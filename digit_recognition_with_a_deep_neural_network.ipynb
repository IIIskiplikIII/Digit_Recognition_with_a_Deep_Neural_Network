{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "source": [
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "#import seaborn as sn\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "from random import seed\r\n",
    "seed(1)\r\n",
    "seed = 43\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "print(\"Tensorflow Version: \", tf.__version__)\r\n",
    "print(\"Keras Version: \",keras.__version__)\r\n",
    "\r\n",
    "\r\n",
    "kaggle = 0 # Kaggle path active = 1\r\n",
    "\r\n",
    "# change your local path here\r\n",
    "if kaggle == 1 :\r\n",
    "    MNIST_PATH= '../input/digit-recognizer'\r\n",
    "else:\r\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "import os\r\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \r\n",
    "    for filename in filenames:\r\n",
    "        print(os.path.join(dirname, filename))\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "Link to the topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "This is another Notebook to take a look into annother algorithm. Here I want to give the Deep Neural Network with the Framework Keras a try. As already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Another MNIST Try: https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters: https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "source": [
    "# Data path and file\r\n",
    "#MNIST_PATH= '../input/digit-recognizer'\r\n",
    "#MNIST_PATH= '../Another_MNIST_try/data/input/digit-recognizer'\r\n",
    "CSV_FILE_TRAIN='train.csv'\r\n",
    "CSV_FILE_TEST='test.csv'\r\n",
    "\r\n",
    "def load_mnist_data(minist_path, csv_file):\r\n",
    "    csv_path = os.path.join(minist_path, csv_file)\r\n",
    "    return pd.read_csv(csv_path)\r\n",
    "\r\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\r\n",
    "    csv_path = os.path.join(minist_path, csv_file)\r\n",
    "    csv_file = open(csv_path, 'r')\r\n",
    "    csv_data = csv_file.readlines()\r\n",
    "    csv_file.close()\r\n",
    "    return csv_data\r\n",
    "\r\n",
    "def split_train_val(data, val_ratio):\r\n",
    "    return \r\n",
    "    \r\n",
    "\r\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\r\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "source": [
    "y = train['label'].copy()\r\n",
    "X = train.drop(['label'], axis=1)\r\n",
    "\r\n",
    "X_test = test.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train / Val Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "source": [
    "print(\"Shape of the Features: \",X.shape)\r\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TK - Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "source": [
    "train.value_counts('label')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 363
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.15\r\n",
    "                                                  , stratify=y\r\n",
    "                                                 )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "source": [
    "print(\"Train - Set Distribution\")\r\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\r\n",
    "print('--------------------------------------------------------------')\r\n",
    "print('--------------------------------------------------------------')\r\n",
    "print('--------------------------------------------------------------')\r\n",
    "print(\"Val - Set Distribution\")\r\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111513\n",
      "7    0.104790\n",
      "3    0.103585\n",
      "9    0.099720\n",
      "2    0.099440\n",
      "6    0.098515\n",
      "0    0.098375\n",
      "4    0.096947\n",
      "8    0.096751\n",
      "5    0.090364\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111587\n",
      "7    0.104762\n",
      "3    0.103651\n",
      "9    0.099683\n",
      "2    0.099524\n",
      "6    0.098413\n",
      "0    0.098413\n",
      "4    0.096984\n",
      "8    0.096667\n",
      "5    0.090317\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "source": [
    "print(\"X: \", X.shape)\r\n",
    "print(\"X_train: \", X_train.shape)\r\n",
    "print(\"X_val: \", X_val.shape)\r\n",
    "\r\n",
    "print(\"y_train: \", y_train.shape)\r\n",
    "print(\"y_val: \", y_val.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (35700, 784)\n",
      "X_val:  (6300, 784)\n",
      "y_train:  (35700,)\n",
      "y_val:  (6300,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TK - Free space for coding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "source": [
    "#############################################################\r\n",
    "\r\n",
    "plt.imshow(np.asfarray(X_train[2:3]).reshape(28,28), cmap='Greys')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0e2d2cc70>"
      ]
     },
     "metadata": {},
     "execution_count": 367
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-27T18:05:53.278944</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p8d9d30fb14)\">\r\n    <image height=\"218\" id=\"image975e899c19\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m205987f876\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m205987f876\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m205987f876\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m205987f876\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m205987f876\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m205987f876\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m205987f876\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mb1d1d2caf7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb1d1d2caf7\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb1d1d2caf7\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb1d1d2caf7\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb1d1d2caf7\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb1d1d2caf7\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb1d1d2caf7\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p8d9d30fb14\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Transforming Piplines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "source": [
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import Normalizer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "pipeline = Pipeline([\r\n",
    "    ('normalizer', Normalizer())\r\n",
    "    #('std_scalar',StandardScaler())\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "source": [
    "X_train_prep = pipeline.fit_transform(X_train)      # fitting the pipeline to the train and transform it\r\n",
    "X_val_prep = pipeline.transform(X_val)              # transform val data with this information"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Deep Neural Network based on RandomizedSearch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\r\n",
    "\r\n",
    "print(\"Relative root_logdir: \",root_logdir)\r\n",
    "\r\n",
    "def get_run_logdir():\r\n",
    "    import time\r\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\r\n",
    "    return os.path.join(root_logdir,run_id)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "source": [
    "run_logdir = get_run_logdir()\r\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_08_27-18_05_53\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "source": [
    "run_logdir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_08_27-18_05_53'"
      ]
     },
     "metadata": {},
     "execution_count": 372
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Model Architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Architecture for Hyperparameter Optimization\n",
    "- Amount of Layers\n",
    "- Amount of Neurons\n",
    "- Learningrate\n",
    "- Checkpoints !!!!!!!!!!!!!!!!!!!!!\n",
    "- Early Stopping "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[784]):\r\n",
    "    model = keras.models.Sequential()                               # base model structure (Sequential API by Keras)\r\n",
    "\r\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))     # input layer\r\n",
    "\r\n",
    "    for layer in range(n_hidden):                                   # add layers as often as defined in constructor \r\n",
    "        model.add(keras.layers.Dense(n_neurons,activation=\"relu\"))  # add layer with given neurons and relu activation function\r\n",
    "\r\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))                               # add output layer \r\n",
    "\r\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)   # define optimizer (especially the larning rate for hyperparameter optimization)\r\n",
    "\r\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])                  # make it ready\r\n",
    "\r\n",
    "    return model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "source": [
    "# Using keras wrapper as hull \r\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Space"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "source": [
    "from scipy.stats import reciprocal\r\n",
    "\r\n",
    "\r\n",
    "# Hyperparameter set\r\n",
    "param_dist= {\r\n",
    "            \"n_neurons\": range(20, 500, 20)\r\n",
    "            ,\"n_hidden\": range(10, 100, 10)\r\n",
    "            ,\"learning_rate\": [1e-3, 2e-3]\r\n",
    "    }\r\n",
    "\r\n",
    "\r\n",
    "param_dist_lr= {\r\n",
    "        \"n_neurons\": [10, 50, 100, 150, 300]                   #[10]     153 sekunden                                  #  [10]  #133 sekunden          ##[10,50,150,300]                     #range(10,300,10) \r\n",
    "        ,\"n_hidden\": [10, 50, 100, 150]                  #[10]     verlust höher und genauigkeit geringer        #  [10]  #                           ##[10,40,80]                          #range(10, 100, 10)\r\n",
    "        ,\"learning_rate\": [1e-3, 3e-4, 3e-2]               #[1e-3]                                                 #  [1e-2] #                 ##[1e-3, 1e-2]    # [np.exp(np.log(10**6)/1000)]   #### exp(log(10**6)/500)\r\n",
    "                                                                        # np.reciprocal(3e-4, 3e-2)\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "param_dist_bestrun_1 = {\r\n",
    "        \"n_neurons\": [150]\r\n",
    "        ,\"n_hidden\": [30]\r\n",
    "        ,\"learning_rate\": [2e-3]  \r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "param_dist_bestrun_2 = {\r\n",
    "        \"n_neurons\": [100]\r\n",
    "        ,\"n_hidden\": [10]\r\n",
    "        ,\"learning_rate\": [2e-3]  \r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TK - Model Checkpoints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_rans_model.h5\", save_best_only=True, save_weights_only=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Randomized Search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "source": [
    "# Finding best hyperparameters with Randomized search\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "\r\n",
    "ran_ker_reg = RandomizedSearchCV(keras_reg, param_dist_lr, n_iter=10, cv=3, random_state=seed, return_train_score=True, n_jobs=5)\r\n",
    "history_ker_reg = ran_ker_reg.fit(X_train_prep, y_train, epochs=100, validation_data=(X_val_prep, y_val), callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=5), tensorboard_cb])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 3:44 - loss: 2.3027 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.6025s). Check your callbacks.\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 2.3023 - accuracy: 0.1192 - val_loss: 2.3020 - val_accuracy: 0.1243\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 767us/step - loss: 2.3016 - accuracy: 0.1180 - val_loss: 2.3011 - val_accuracy: 0.1173\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 790us/step - loss: 2.3007 - accuracy: 0.1142 - val_loss: 2.3002 - val_accuracy: 0.1132\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 750us/step - loss: 2.2997 - accuracy: 0.1122 - val_loss: 2.2992 - val_accuracy: 0.1067\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 774us/step - loss: 2.2986 - accuracy: 0.1063 - val_loss: 2.2980 - val_accuracy: 0.1057\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 773us/step - loss: 2.2973 - accuracy: 0.1002 - val_loss: 2.2966 - val_accuracy: 0.0984\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 808us/step - loss: 2.2958 - accuracy: 0.0961 - val_loss: 2.2950 - val_accuracy: 0.0943\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 793us/step - loss: 2.2940 - accuracy: 0.0916 - val_loss: 2.2930 - val_accuracy: 0.0895\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 773us/step - loss: 2.2919 - accuracy: 0.0897 - val_loss: 2.2907 - val_accuracy: 0.0875\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 766us/step - loss: 2.2894 - accuracy: 0.1045 - val_loss: 2.2879 - val_accuracy: 0.1070\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 764us/step - loss: 2.2864 - accuracy: 0.1168 - val_loss: 2.2846 - val_accuracy: 0.1214\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 776us/step - loss: 2.2828 - accuracy: 0.1254 - val_loss: 2.2806 - val_accuracy: 0.1322\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 764us/step - loss: 2.2784 - accuracy: 0.1331 - val_loss: 2.2756 - val_accuracy: 0.1370\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 770us/step - loss: 2.2729 - accuracy: 0.1400 - val_loss: 2.2693 - val_accuracy: 0.1438\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 769us/step - loss: 2.2658 - accuracy: 0.1462 - val_loss: 2.2612 - val_accuracy: 0.1494\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 757us/step - loss: 2.2569 - accuracy: 0.1518 - val_loss: 2.2510 - val_accuracy: 0.1517\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 790us/step - loss: 2.2455 - accuracy: 0.1552 - val_loss: 2.2380 - val_accuracy: 0.1573\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 784us/step - loss: 2.2310 - accuracy: 0.1614 - val_loss: 2.2216 - val_accuracy: 0.1621\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 820us/step - loss: 2.2129 - accuracy: 0.1797 - val_loss: 2.2017 - val_accuracy: 0.1789\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 794us/step - loss: 2.1909 - accuracy: 0.1789 - val_loss: 2.1768 - val_accuracy: 0.1771\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 810us/step - loss: 2.1654 - accuracy: 0.1766 - val_loss: 2.1496 - val_accuracy: 0.1790\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 810us/step - loss: 2.1376 - accuracy: 0.1781 - val_loss: 2.1211 - val_accuracy: 0.1751\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 802us/step - loss: 2.1098 - accuracy: 0.1792 - val_loss: 2.0939 - val_accuracy: 0.1808\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 776us/step - loss: 2.0831 - accuracy: 0.1809 - val_loss: 2.0679 - val_accuracy: 0.1794\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 833us/step - loss: 2.0585 - accuracy: 0.1821 - val_loss: 2.0445 - val_accuracy: 0.1802\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 904us/step - loss: 2.0353 - accuracy: 0.1834 - val_loss: 2.0249 - val_accuracy: 0.1767\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 905us/step - loss: 2.0132 - accuracy: 0.1845 - val_loss: 2.0004 - val_accuracy: 0.1908\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 774us/step - loss: 1.9924 - accuracy: 0.1876 - val_loss: 1.9792 - val_accuracy: 0.1921\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 802us/step - loss: 1.9724 - accuracy: 0.1913 - val_loss: 1.9596 - val_accuracy: 0.1952\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 788us/step - loss: 1.9531 - accuracy: 0.1991 - val_loss: 1.9446 - val_accuracy: 0.2187\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 802us/step - loss: 1.9340 - accuracy: 0.2075 - val_loss: 1.9233 - val_accuracy: 0.2232\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 798us/step - loss: 1.9145 - accuracy: 0.2181 - val_loss: 1.9021 - val_accuracy: 0.2271\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 788us/step - loss: 1.8941 - accuracy: 0.2265 - val_loss: 1.8801 - val_accuracy: 0.2306\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 786us/step - loss: 1.8719 - accuracy: 0.2365 - val_loss: 1.8580 - val_accuracy: 0.2446\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 763us/step - loss: 1.8482 - accuracy: 0.2416 - val_loss: 1.8351 - val_accuracy: 0.2686\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 766us/step - loss: 1.8240 - accuracy: 0.2499 - val_loss: 1.8092 - val_accuracy: 0.2646\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 768us/step - loss: 1.7989 - accuracy: 0.2646 - val_loss: 1.7852 - val_accuracy: 0.2795\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 790us/step - loss: 1.7730 - accuracy: 0.2698 - val_loss: 1.7589 - val_accuracy: 0.2911\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 804us/step - loss: 1.7481 - accuracy: 0.2863 - val_loss: 1.7330 - val_accuracy: 0.2929\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 767us/step - loss: 1.7218 - accuracy: 0.2866 - val_loss: 1.7116 - val_accuracy: 0.2884\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 778us/step - loss: 1.6953 - accuracy: 0.2980 - val_loss: 1.6860 - val_accuracy: 0.2900\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 804us/step - loss: 1.6702 - accuracy: 0.3178 - val_loss: 1.6561 - val_accuracy: 0.3343\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 768us/step - loss: 1.6442 - accuracy: 0.3252 - val_loss: 1.6303 - val_accuracy: 0.3390\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 773us/step - loss: 1.6184 - accuracy: 0.3350 - val_loss: 1.6054 - val_accuracy: 0.3371\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 788us/step - loss: 1.5934 - accuracy: 0.3450 - val_loss: 1.5899 - val_accuracy: 0.3463\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 822us/step - loss: 1.5678 - accuracy: 0.3541 - val_loss: 1.5590 - val_accuracy: 0.3617\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 788us/step - loss: 1.5428 - accuracy: 0.3664 - val_loss: 1.5629 - val_accuracy: 0.3400\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 829us/step - loss: 1.5148 - accuracy: 0.3782 - val_loss: 1.5105 - val_accuracy: 0.3870\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 856us/step - loss: 1.4868 - accuracy: 0.3930 - val_loss: 1.4812 - val_accuracy: 0.3927\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 890us/step - loss: 1.4612 - accuracy: 0.4117 - val_loss: 1.4517 - val_accuracy: 0.4273\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 861us/step - loss: 1.4324 - accuracy: 0.4220 - val_loss: 1.4322 - val_accuracy: 0.4265\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 866us/step - loss: 1.4008 - accuracy: 0.4379 - val_loss: 1.4145 - val_accuracy: 0.4302\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 812us/step - loss: 1.3749 - accuracy: 0.4495 - val_loss: 1.3709 - val_accuracy: 0.4613\n",
      "Epoch 54/100\n",
      "744/744 [==============================] - 1s 767us/step - loss: 1.3474 - accuracy: 0.4650 - val_loss: 1.3472 - val_accuracy: 0.4687\n",
      "Epoch 55/100\n",
      "744/744 [==============================] - 1s 777us/step - loss: 1.3250 - accuracy: 0.4758 - val_loss: 1.3164 - val_accuracy: 0.5008\n",
      "Epoch 56/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 1.3020 - accuracy: 0.4887 - val_loss: 1.5067 - val_accuracy: 0.3808\n",
      "Epoch 57/100\n",
      "744/744 [==============================] - 1s 837us/step - loss: 1.2783 - accuracy: 0.4973 - val_loss: 1.3703 - val_accuracy: 0.4651\n",
      "Epoch 58/100\n",
      "744/744 [==============================] - 1s 876us/step - loss: 1.2578 - accuracy: 0.5087 - val_loss: 1.2509 - val_accuracy: 0.5316\n",
      "Epoch 59/100\n",
      "744/744 [==============================] - 1s 839us/step - loss: 1.2395 - accuracy: 0.5249 - val_loss: 1.2313 - val_accuracy: 0.5375\n",
      "Epoch 60/100\n",
      "744/744 [==============================] - 1s 808us/step - loss: 1.2225 - accuracy: 0.5328 - val_loss: 1.2175 - val_accuracy: 0.5524\n",
      "Epoch 61/100\n",
      "744/744 [==============================] - 1s 846us/step - loss: 1.2003 - accuracy: 0.5446 - val_loss: 1.2940 - val_accuracy: 0.5124\n",
      "Epoch 62/100\n",
      "744/744 [==============================] - 1s 929us/step - loss: 1.1823 - accuracy: 0.5570 - val_loss: 1.1804 - val_accuracy: 0.5844\n",
      "Epoch 63/100\n",
      "744/744 [==============================] - 1s 845us/step - loss: 1.1681 - accuracy: 0.5703 - val_loss: 1.1609 - val_accuracy: 0.5979\n",
      "Epoch 64/100\n",
      "744/744 [==============================] - 1s 786us/step - loss: 1.1524 - accuracy: 0.5844 - val_loss: 1.1542 - val_accuracy: 0.6051\n",
      "Epoch 65/100\n",
      "744/744 [==============================] - 1s 760us/step - loss: 1.1281 - accuracy: 0.6017 - val_loss: 1.1589 - val_accuracy: 0.5959\n",
      "Epoch 66/100\n",
      "744/744 [==============================] - 1s 764us/step - loss: 1.1137 - accuracy: 0.6045 - val_loss: 1.1102 - val_accuracy: 0.6295\n",
      "Epoch 67/100\n",
      "744/744 [==============================] - 1s 757us/step - loss: 1.0956 - accuracy: 0.6177 - val_loss: 1.2466 - val_accuracy: 0.5614\n",
      "Epoch 68/100\n",
      "744/744 [==============================] - 1s 803us/step - loss: 1.0784 - accuracy: 0.6265 - val_loss: 1.1059 - val_accuracy: 0.6321\n",
      "Epoch 69/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 1.0714 - accuracy: 0.6271 - val_loss: 1.1114 - val_accuracy: 0.6235\n",
      "Epoch 70/100\n",
      "744/744 [==============================] - 1s 815us/step - loss: 1.0562 - accuracy: 0.6358 - val_loss: 1.0699 - val_accuracy: 0.6548\n",
      "Epoch 71/100\n",
      "744/744 [==============================] - 1s 769us/step - loss: 1.0449 - accuracy: 0.6386 - val_loss: 1.1850 - val_accuracy: 0.5779\n",
      "Epoch 72/100\n",
      "744/744 [==============================] - 1s 778us/step - loss: 1.0366 - accuracy: 0.6483 - val_loss: 1.0380 - val_accuracy: 0.6725\n",
      "Epoch 73/100\n",
      "744/744 [==============================] - 1s 826us/step - loss: 1.0205 - accuracy: 0.6545 - val_loss: 1.0362 - val_accuracy: 0.6746\n",
      "Epoch 74/100\n",
      "744/744 [==============================] - 1s 868us/step - loss: 1.0229 - accuracy: 0.6545 - val_loss: 1.1567 - val_accuracy: 0.5978\n",
      "Epoch 75/100\n",
      "744/744 [==============================] - 1s 867us/step - loss: 1.0016 - accuracy: 0.6619 - val_loss: 1.0225 - val_accuracy: 0.6737\n",
      "Epoch 76/100\n",
      "744/744 [==============================] - 1s 864us/step - loss: 0.9951 - accuracy: 0.6674 - val_loss: 1.0105 - val_accuracy: 0.6884\n",
      "Epoch 77/100\n",
      "744/744 [==============================] - 1s 824us/step - loss: 0.9809 - accuracy: 0.6795 - val_loss: 1.0405 - val_accuracy: 0.6714\n",
      "Epoch 78/100\n",
      "744/744 [==============================] - 1s 798us/step - loss: 0.9781 - accuracy: 0.6777 - val_loss: 1.0242 - val_accuracy: 0.6786\n",
      "Epoch 79/100\n",
      "744/744 [==============================] - 1s 874us/step - loss: 0.9640 - accuracy: 0.6829 - val_loss: 1.0491 - val_accuracy: 0.6592\n",
      "Epoch 80/100\n",
      "744/744 [==============================] - 1s 884us/step - loss: 0.9563 - accuracy: 0.6895 - val_loss: 0.9994 - val_accuracy: 0.6841\n",
      "Epoch 81/100\n",
      "744/744 [==============================] - 1s 804us/step - loss: 0.9519 - accuracy: 0.6881 - val_loss: 0.9715 - val_accuracy: 0.7016\n",
      "Epoch 82/100\n",
      "744/744 [==============================] - 1s 779us/step - loss: 0.9464 - accuracy: 0.6904 - val_loss: 1.0074 - val_accuracy: 0.6941\n",
      "Epoch 83/100\n",
      "744/744 [==============================] - 1s 872us/step - loss: 0.9324 - accuracy: 0.6999 - val_loss: 0.9829 - val_accuracy: 0.7113\n",
      "Epoch 84/100\n",
      "744/744 [==============================] - 1s 907us/step - loss: 0.9299 - accuracy: 0.7028 - val_loss: 0.9631 - val_accuracy: 0.7252\n",
      "Epoch 85/100\n",
      "744/744 [==============================] - 1s 829us/step - loss: 0.9193 - accuracy: 0.7068 - val_loss: 0.9462 - val_accuracy: 0.7295\n",
      "Epoch 86/100\n",
      "744/744 [==============================] - 1s 772us/step - loss: 0.9139 - accuracy: 0.7049 - val_loss: 1.0003 - val_accuracy: 0.7011\n",
      "Epoch 87/100\n",
      "744/744 [==============================] - 1s 768us/step - loss: 0.9060 - accuracy: 0.7129 - val_loss: 0.9729 - val_accuracy: 0.7040\n",
      "Epoch 88/100\n",
      "744/744 [==============================] - 1s 774us/step - loss: 0.8929 - accuracy: 0.7155 - val_loss: 0.9486 - val_accuracy: 0.7314\n",
      "Epoch 89/100\n",
      "744/744 [==============================] - 1s 833us/step - loss: 0.8932 - accuracy: 0.7210 - val_loss: 0.9263 - val_accuracy: 0.7368\n",
      "Epoch 90/100\n",
      "744/744 [==============================] - 1s 790us/step - loss: 0.8896 - accuracy: 0.7161 - val_loss: 0.9555 - val_accuracy: 0.7221\n",
      "Epoch 91/100\n",
      "744/744 [==============================] - 1s 761us/step - loss: 0.8805 - accuracy: 0.7215 - val_loss: 0.9744 - val_accuracy: 0.7144\n",
      "Epoch 92/100\n",
      "744/744 [==============================] - 1s 769us/step - loss: 0.8689 - accuracy: 0.7258 - val_loss: 0.9475 - val_accuracy: 0.7092\n",
      "Epoch 93/100\n",
      "744/744 [==============================] - 1s 792us/step - loss: 0.8773 - accuracy: 0.7226 - val_loss: 0.9143 - val_accuracy: 0.7317\n",
      "Epoch 94/100\n",
      "744/744 [==============================] - 1s 770us/step - loss: 0.8611 - accuracy: 0.7291 - val_loss: 0.9487 - val_accuracy: 0.6968\n",
      "Epoch 95/100\n",
      "744/744 [==============================] - 1s 810us/step - loss: 0.8574 - accuracy: 0.7332 - val_loss: 0.9705 - val_accuracy: 0.7094\n",
      "Epoch 96/100\n",
      "744/744 [==============================] - 1s 861us/step - loss: 0.8551 - accuracy: 0.7341 - val_loss: 0.8991 - val_accuracy: 0.7497\n",
      "Epoch 97/100\n",
      "744/744 [==============================] - 1s 821us/step - loss: 0.8422 - accuracy: 0.7413 - val_loss: 0.8850 - val_accuracy: 0.7405\n",
      "Epoch 98/100\n",
      "744/744 [==============================] - 1s 829us/step - loss: 0.8384 - accuracy: 0.7435 - val_loss: 0.8868 - val_accuracy: 0.7529\n",
      "Epoch 99/100\n",
      "744/744 [==============================] - 1s 776us/step - loss: 0.8400 - accuracy: 0.7394 - val_loss: 0.9239 - val_accuracy: 0.7283\n",
      "Epoch 100/100\n",
      "744/744 [==============================] - 1s 779us/step - loss: 0.8206 - accuracy: 0.7515 - val_loss: 0.9174 - val_accuracy: 0.7311\n",
      "372/372 [==============================] - 0s 452us/step - loss: 0.9025 - accuracy: 0.7388\n",
      "744/744 [==============================] - 0s 450us/step - loss: 0.8174 - accuracy: 0.7465\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:09 - loss: 2.3025 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.3485s). Check your callbacks.\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.3023 - accuracy: 0.1384 - val_loss: 2.3019 - val_accuracy: 0.1617\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 2.3016 - accuracy: 0.1548 - val_loss: 2.3012 - val_accuracy: 0.1575\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 744us/step - loss: 2.3009 - accuracy: 0.1576 - val_loss: 2.3006 - val_accuracy: 0.1505\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 769us/step - loss: 2.3003 - accuracy: 0.1547 - val_loss: 2.3000 - val_accuracy: 0.1481\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 760us/step - loss: 2.2997 - accuracy: 0.1540 - val_loss: 2.2993 - val_accuracy: 0.1378\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 751us/step - loss: 2.2990 - accuracy: 0.1504 - val_loss: 2.2986 - val_accuracy: 0.1414\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 733us/step - loss: 2.2983 - accuracy: 0.1445 - val_loss: 2.2978 - val_accuracy: 0.1468\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 753us/step - loss: 2.2974 - accuracy: 0.1480 - val_loss: 2.2970 - val_accuracy: 0.1329\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 759us/step - loss: 2.2965 - accuracy: 0.1471 - val_loss: 2.2959 - val_accuracy: 0.1470\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 731us/step - loss: 2.2954 - accuracy: 0.1500 - val_loss: 2.2947 - val_accuracy: 0.1479\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 796us/step - loss: 2.2941 - accuracy: 0.1529 - val_loss: 2.2933 - val_accuracy: 0.1543\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 768us/step - loss: 2.2926 - accuracy: 0.1590 - val_loss: 2.2917 - val_accuracy: 0.1643\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 791us/step - loss: 2.2909 - accuracy: 0.1645 - val_loss: 2.2898 - val_accuracy: 0.1705\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 774us/step - loss: 2.2888 - accuracy: 0.1727 - val_loss: 2.2876 - val_accuracy: 0.1748\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 767us/step - loss: 2.2864 - accuracy: 0.1815 - val_loss: 2.2850 - val_accuracy: 0.1829\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 753us/step - loss: 2.2837 - accuracy: 0.2156 - val_loss: 2.2822 - val_accuracy: 0.2321\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 732us/step - loss: 2.2804 - accuracy: 0.2298 - val_loss: 2.2785 - val_accuracy: 0.2281\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 2.2766 - accuracy: 0.2275 - val_loss: 2.2743 - val_accuracy: 0.2241\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 739us/step - loss: 2.2719 - accuracy: 0.2250 - val_loss: 2.2691 - val_accuracy: 0.2214\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 754us/step - loss: 2.2663 - accuracy: 0.2184 - val_loss: 2.2631 - val_accuracy: 0.2165\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 731us/step - loss: 2.2597 - accuracy: 0.2184 - val_loss: 2.2560 - val_accuracy: 0.2159\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 727us/step - loss: 2.2519 - accuracy: 0.2142 - val_loss: 2.2477 - val_accuracy: 0.2149\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 737us/step - loss: 2.2430 - accuracy: 0.2120 - val_loss: 2.2382 - val_accuracy: 0.2087\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 759us/step - loss: 2.2327 - accuracy: 0.2065 - val_loss: 2.2273 - val_accuracy: 0.2086\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 737us/step - loss: 2.2211 - accuracy: 0.2065 - val_loss: 2.2150 - val_accuracy: 0.2086\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 763us/step - loss: 2.2079 - accuracy: 0.2065 - val_loss: 2.2013 - val_accuracy: 0.2089\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 745us/step - loss: 2.1934 - accuracy: 0.2065 - val_loss: 2.1862 - val_accuracy: 0.2089\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 728us/step - loss: 2.1774 - accuracy: 0.2066 - val_loss: 2.1698 - val_accuracy: 0.2083\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 731us/step - loss: 2.1604 - accuracy: 0.2066 - val_loss: 2.1522 - val_accuracy: 0.2078\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 745us/step - loss: 2.1418 - accuracy: 0.2064 - val_loss: 2.1333 - val_accuracy: 0.2078\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 734us/step - loss: 2.1222 - accuracy: 0.2067 - val_loss: 2.1136 - val_accuracy: 0.2095\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 722us/step - loss: 2.1026 - accuracy: 0.2079 - val_loss: 2.0946 - val_accuracy: 0.2098\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 729us/step - loss: 2.0830 - accuracy: 0.2091 - val_loss: 2.0760 - val_accuracy: 0.2105\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 754us/step - loss: 2.0638 - accuracy: 0.2108 - val_loss: 2.0569 - val_accuracy: 0.2119\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 738us/step - loss: 2.0448 - accuracy: 0.2114 - val_loss: 2.0393 - val_accuracy: 0.2121\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 2.0262 - accuracy: 0.2121 - val_loss: 2.0213 - val_accuracy: 0.2144\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 745us/step - loss: 2.0079 - accuracy: 0.2157 - val_loss: 2.0038 - val_accuracy: 0.2170\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 727us/step - loss: 1.9904 - accuracy: 0.2193 - val_loss: 1.9862 - val_accuracy: 0.2183\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 756us/step - loss: 1.9725 - accuracy: 0.2172 - val_loss: 1.9687 - val_accuracy: 0.2362\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 740us/step - loss: 1.9545 - accuracy: 0.2394 - val_loss: 1.9507 - val_accuracy: 0.2389\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 728us/step - loss: 1.9359 - accuracy: 0.2467 - val_loss: 1.9322 - val_accuracy: 0.2497\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 737us/step - loss: 1.9161 - accuracy: 0.2542 - val_loss: 1.9162 - val_accuracy: 0.2575\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 740us/step - loss: 1.8945 - accuracy: 0.2626 - val_loss: 1.8889 - val_accuracy: 0.2662\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 753us/step - loss: 1.8706 - accuracy: 0.2719 - val_loss: 1.8667 - val_accuracy: 0.2740\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 738us/step - loss: 1.8444 - accuracy: 0.2846 - val_loss: 1.8367 - val_accuracy: 0.2925\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 750us/step - loss: 1.8154 - accuracy: 0.2971 - val_loss: 1.8065 - val_accuracy: 0.2963\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 1.7828 - accuracy: 0.3059 - val_loss: 1.7853 - val_accuracy: 0.3048\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 736us/step - loss: 1.7473 - accuracy: 0.3195 - val_loss: 1.7370 - val_accuracy: 0.3146\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 733us/step - loss: 1.7084 - accuracy: 0.3266 - val_loss: 1.6975 - val_accuracy: 0.3314\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 721us/step - loss: 1.6684 - accuracy: 0.3389 - val_loss: 1.6646 - val_accuracy: 0.3384\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 745us/step - loss: 1.6283 - accuracy: 0.3495 - val_loss: 1.6166 - val_accuracy: 0.3448\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 785us/step - loss: 1.5892 - accuracy: 0.3585 - val_loss: 1.5797 - val_accuracy: 0.3627\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 745us/step - loss: 1.5512 - accuracy: 0.3660 - val_loss: 1.5464 - val_accuracy: 0.3719\n",
      "Epoch 54/100\n",
      "744/744 [==============================] - 1s 750us/step - loss: 1.5186 - accuracy: 0.3788 - val_loss: 1.5108 - val_accuracy: 0.3829\n",
      "Epoch 55/100\n",
      "744/744 [==============================] - 1s 746us/step - loss: 1.4875 - accuracy: 0.3957 - val_loss: 1.4845 - val_accuracy: 0.4030\n",
      "Epoch 56/100\n",
      "744/744 [==============================] - 1s 722us/step - loss: 1.4599 - accuracy: 0.4230 - val_loss: 1.4625 - val_accuracy: 0.4298\n",
      "Epoch 57/100\n",
      "744/744 [==============================] - 1s 743us/step - loss: 1.4345 - accuracy: 0.4405 - val_loss: 1.5472 - val_accuracy: 0.3598\n",
      "Epoch 58/100\n",
      "744/744 [==============================] - 1s 724us/step - loss: 1.4091 - accuracy: 0.4553 - val_loss: 1.4090 - val_accuracy: 0.4690\n",
      "Epoch 59/100\n",
      "744/744 [==============================] - 1s 716us/step - loss: 1.3880 - accuracy: 0.4676 - val_loss: 1.3982 - val_accuracy: 0.4690\n",
      "Epoch 60/100\n",
      "744/744 [==============================] - 1s 725us/step - loss: 1.3639 - accuracy: 0.4766 - val_loss: 1.3722 - val_accuracy: 0.4954\n",
      "Epoch 61/100\n",
      "744/744 [==============================] - 1s 735us/step - loss: 1.3454 - accuracy: 0.4898 - val_loss: 1.3451 - val_accuracy: 0.5013\n",
      "Epoch 62/100\n",
      "744/744 [==============================] - 1s 739us/step - loss: 1.3231 - accuracy: 0.5011 - val_loss: 1.3433 - val_accuracy: 0.4992\n",
      "Epoch 63/100\n",
      "744/744 [==============================] - 1s 751us/step - loss: 1.3053 - accuracy: 0.5089 - val_loss: 1.3545 - val_accuracy: 0.4829\n",
      "Epoch 64/100\n",
      "744/744 [==============================] - 1s 756us/step - loss: 1.2893 - accuracy: 0.5187 - val_loss: 1.3641 - val_accuracy: 0.4687\n",
      "Epoch 65/100\n",
      "744/744 [==============================] - 1s 730us/step - loss: 1.2722 - accuracy: 0.5229 - val_loss: 1.3652 - val_accuracy: 0.4648\n",
      "Epoch 66/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 1.2597 - accuracy: 0.5308 - val_loss: 1.2747 - val_accuracy: 0.5314\n",
      "Epoch 67/100\n",
      "744/744 [==============================] - 1s 727us/step - loss: 1.2496 - accuracy: 0.5377 - val_loss: 1.2977 - val_accuracy: 0.4946\n",
      "Epoch 68/100\n",
      "744/744 [==============================] - 1s 724us/step - loss: 1.2303 - accuracy: 0.5438 - val_loss: 1.2378 - val_accuracy: 0.5527\n",
      "Epoch 69/100\n",
      "744/744 [==============================] - 1s 726us/step - loss: 1.2209 - accuracy: 0.5479 - val_loss: 1.2581 - val_accuracy: 0.5305\n",
      "Epoch 70/100\n",
      "744/744 [==============================] - 1s 726us/step - loss: 1.2062 - accuracy: 0.5524 - val_loss: 1.2112 - val_accuracy: 0.5651\n",
      "Epoch 71/100\n",
      "744/744 [==============================] - 1s 722us/step - loss: 1.1987 - accuracy: 0.5545 - val_loss: 1.2075 - val_accuracy: 0.5632\n",
      "Epoch 72/100\n",
      "744/744 [==============================] - 1s 749us/step - loss: 1.1894 - accuracy: 0.5650 - val_loss: 1.2127 - val_accuracy: 0.5584\n",
      "Epoch 73/100\n",
      "744/744 [==============================] - 1s 733us/step - loss: 1.1744 - accuracy: 0.5722 - val_loss: 1.2867 - val_accuracy: 0.5103\n",
      "Epoch 74/100\n",
      "744/744 [==============================] - 1s 776us/step - loss: 1.1682 - accuracy: 0.5742 - val_loss: 1.2490 - val_accuracy: 0.5224\n",
      "Epoch 75/100\n",
      "744/744 [==============================] - 1s 728us/step - loss: 1.1534 - accuracy: 0.5861 - val_loss: 1.1514 - val_accuracy: 0.6070\n",
      "Epoch 76/100\n",
      "744/744 [==============================] - 1s 726us/step - loss: 1.1431 - accuracy: 0.5871 - val_loss: 1.1441 - val_accuracy: 0.6122\n",
      "Epoch 77/100\n",
      "744/744 [==============================] - 1s 739us/step - loss: 1.1308 - accuracy: 0.5966 - val_loss: 1.1600 - val_accuracy: 0.5876\n",
      "Epoch 78/100\n",
      "744/744 [==============================] - 1s 735us/step - loss: 1.1163 - accuracy: 0.6054 - val_loss: 1.1570 - val_accuracy: 0.6084\n",
      "Epoch 79/100\n",
      "744/744 [==============================] - 1s 733us/step - loss: 1.1108 - accuracy: 0.6082 - val_loss: 1.1776 - val_accuracy: 0.5784\n",
      "Epoch 80/100\n",
      "744/744 [==============================] - 1s 724us/step - loss: 1.1006 - accuracy: 0.6142 - val_loss: 1.1736 - val_accuracy: 0.5683\n",
      "Epoch 81/100\n",
      "744/744 [==============================] - 1s 719us/step - loss: 1.0903 - accuracy: 0.6229 - val_loss: 1.0889 - val_accuracy: 0.6481\n",
      "Epoch 82/100\n",
      "744/744 [==============================] - 1s 731us/step - loss: 1.0872 - accuracy: 0.6234 - val_loss: 1.0807 - val_accuracy: 0.6492\n",
      "Epoch 83/100\n",
      "744/744 [==============================] - 1s 735us/step - loss: 1.0736 - accuracy: 0.6282 - val_loss: 1.1606 - val_accuracy: 0.5867\n",
      "Epoch 84/100\n",
      "744/744 [==============================] - 1s 780us/step - loss: 1.0631 - accuracy: 0.6361 - val_loss: 1.0654 - val_accuracy: 0.6568\n",
      "Epoch 85/100\n",
      "744/744 [==============================] - 1s 734us/step - loss: 1.0550 - accuracy: 0.6400 - val_loss: 1.0871 - val_accuracy: 0.6403\n",
      "Epoch 86/100\n",
      "744/744 [==============================] - 1s 737us/step - loss: 1.0397 - accuracy: 0.6499 - val_loss: 1.0789 - val_accuracy: 0.6463\n",
      "Epoch 87/100\n",
      "744/744 [==============================] - 1s 779us/step - loss: 1.0344 - accuracy: 0.6528 - val_loss: 1.0617 - val_accuracy: 0.6592\n",
      "Epoch 88/100\n",
      "744/744 [==============================] - 1s 757us/step - loss: 1.0261 - accuracy: 0.6585 - val_loss: 1.0661 - val_accuracy: 0.6598\n",
      "Epoch 89/100\n",
      "744/744 [==============================] - 1s 763us/step - loss: 1.0137 - accuracy: 0.6637 - val_loss: 1.0248 - val_accuracy: 0.6783\n",
      "Epoch 90/100\n",
      "744/744 [==============================] - 1s 784us/step - loss: 1.0081 - accuracy: 0.6677 - val_loss: 1.1378 - val_accuracy: 0.6144\n",
      "Epoch 91/100\n",
      "744/744 [==============================] - 1s 769us/step - loss: 0.9980 - accuracy: 0.6704 - val_loss: 1.0406 - val_accuracy: 0.6722\n",
      "Epoch 92/100\n",
      "744/744 [==============================] - 1s 775us/step - loss: 0.9899 - accuracy: 0.6789 - val_loss: 1.0467 - val_accuracy: 0.6683\n",
      "Epoch 93/100\n",
      "744/744 [==============================] - 1s 868us/step - loss: 0.9864 - accuracy: 0.6787 - val_loss: 1.0748 - val_accuracy: 0.6308\n",
      "Epoch 94/100\n",
      "744/744 [==============================] - 1s 792us/step - loss: 0.9749 - accuracy: 0.6852 - val_loss: 1.0180 - val_accuracy: 0.6795\n",
      "Epoch 95/100\n",
      "744/744 [==============================] - 1s 786us/step - loss: 0.9680 - accuracy: 0.6861 - val_loss: 1.0922 - val_accuracy: 0.6165\n",
      "Epoch 96/100\n",
      "744/744 [==============================] - 1s 760us/step - loss: 0.9574 - accuracy: 0.6928 - val_loss: 0.9702 - val_accuracy: 0.7030\n",
      "Epoch 97/100\n",
      "744/744 [==============================] - 1s 757us/step - loss: 0.9520 - accuracy: 0.6950 - val_loss: 1.0463 - val_accuracy: 0.6551\n",
      "Epoch 98/100\n",
      "744/744 [==============================] - 1s 764us/step - loss: 0.9400 - accuracy: 0.7053 - val_loss: 0.9818 - val_accuracy: 0.7011\n",
      "Epoch 99/100\n",
      "744/744 [==============================] - 1s 745us/step - loss: 0.9327 - accuracy: 0.7062 - val_loss: 0.9700 - val_accuracy: 0.7098\n",
      "Epoch 100/100\n",
      "744/744 [==============================] - 1s 770us/step - loss: 0.9248 - accuracy: 0.7058 - val_loss: 0.9741 - val_accuracy: 0.7057\n",
      "372/372 [==============================] - 0s 441us/step - loss: 0.9637 - accuracy: 0.7066\n",
      "744/744 [==============================] - 0s 447us/step - loss: 0.9046 - accuracy: 0.7219\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:02 - loss: 2.3026 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.3295s). Check your callbacks.\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.3024 - accuracy: 0.1115 - val_loss: 2.3023 - val_accuracy: 0.1116\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 812us/step - loss: 2.3021 - accuracy: 0.1224 - val_loss: 2.3019 - val_accuracy: 0.1441\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 779us/step - loss: 2.3017 - accuracy: 0.1495 - val_loss: 2.3016 - val_accuracy: 0.1489\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 743us/step - loss: 2.3014 - accuracy: 0.1577 - val_loss: 2.3012 - val_accuracy: 0.1530\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 747us/step - loss: 2.3010 - accuracy: 0.1599 - val_loss: 2.3007 - val_accuracy: 0.1603\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 747us/step - loss: 2.3005 - accuracy: 0.1610 - val_loss: 2.3003 - val_accuracy: 0.1654\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 754us/step - loss: 2.3000 - accuracy: 0.1729 - val_loss: 2.2998 - val_accuracy: 0.1698\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 727us/step - loss: 2.2995 - accuracy: 0.1784 - val_loss: 2.2993 - val_accuracy: 0.1743\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 761us/step - loss: 2.2990 - accuracy: 0.1782 - val_loss: 2.2988 - val_accuracy: 0.1744\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 791us/step - loss: 2.2985 - accuracy: 0.1789 - val_loss: 2.2983 - val_accuracy: 0.1749\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 757us/step - loss: 2.2979 - accuracy: 0.1858 - val_loss: 2.2977 - val_accuracy: 0.1794\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 819us/step - loss: 2.2973 - accuracy: 0.1861 - val_loss: 2.2971 - val_accuracy: 0.1737\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 775us/step - loss: 2.2966 - accuracy: 0.1840 - val_loss: 2.2963 - val_accuracy: 0.1844\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 767us/step - loss: 2.2958 - accuracy: 0.1875 - val_loss: 2.2955 - val_accuracy: 0.1832\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 807us/step - loss: 2.2950 - accuracy: 0.1892 - val_loss: 2.2947 - val_accuracy: 0.1840\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 749us/step - loss: 2.2940 - accuracy: 0.1915 - val_loss: 2.2937 - val_accuracy: 0.1890\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 747us/step - loss: 2.2930 - accuracy: 0.1932 - val_loss: 2.2926 - val_accuracy: 0.1889\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 764us/step - loss: 2.2918 - accuracy: 0.1961 - val_loss: 2.2913 - val_accuracy: 0.1916\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 797us/step - loss: 2.2903 - accuracy: 0.1969 - val_loss: 2.2898 - val_accuracy: 0.1954\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 738us/step - loss: 2.2887 - accuracy: 0.1992 - val_loss: 2.2881 - val_accuracy: 0.1976\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 788us/step - loss: 2.2868 - accuracy: 0.2004 - val_loss: 2.2860 - val_accuracy: 0.1983\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 756us/step - loss: 2.2844 - accuracy: 0.2022 - val_loss: 2.2835 - val_accuracy: 0.2006\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 772us/step - loss: 2.2817 - accuracy: 0.2030 - val_loss: 2.2805 - val_accuracy: 0.2016\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 743us/step - loss: 2.2783 - accuracy: 0.2045 - val_loss: 2.2769 - val_accuracy: 0.2029\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 765us/step - loss: 2.2742 - accuracy: 0.2058 - val_loss: 2.2725 - val_accuracy: 0.2019\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 756us/step - loss: 2.2692 - accuracy: 0.2066 - val_loss: 2.2671 - val_accuracy: 0.2037\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 757us/step - loss: 2.2632 - accuracy: 0.2073 - val_loss: 2.2606 - val_accuracy: 0.2052\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 789us/step - loss: 2.2555 - accuracy: 0.2079 - val_loss: 2.2517 - val_accuracy: 0.2070\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 810us/step - loss: 2.2450 - accuracy: 0.2082 - val_loss: 2.2403 - val_accuracy: 0.2078\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 784us/step - loss: 2.2325 - accuracy: 0.2084 - val_loss: 2.2271 - val_accuracy: 0.2079\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 791us/step - loss: 2.2181 - accuracy: 0.2088 - val_loss: 2.2119 - val_accuracy: 0.2079\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 782us/step - loss: 2.2015 - accuracy: 0.2088 - val_loss: 2.1943 - val_accuracy: 0.2075\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 848us/step - loss: 2.1825 - accuracy: 0.2089 - val_loss: 2.1746 - val_accuracy: 0.2083\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 755us/step - loss: 2.1613 - accuracy: 0.2091 - val_loss: 2.1526 - val_accuracy: 0.2079\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 773us/step - loss: 2.1384 - accuracy: 0.2094 - val_loss: 2.1294 - val_accuracy: 0.2083\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 775us/step - loss: 2.1146 - accuracy: 0.2093 - val_loss: 2.1061 - val_accuracy: 0.2081\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 776us/step - loss: 2.0911 - accuracy: 0.2093 - val_loss: 2.0835 - val_accuracy: 0.2084\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 766us/step - loss: 2.0687 - accuracy: 0.2095 - val_loss: 2.0620 - val_accuracy: 0.2081\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 748us/step - loss: 2.0480 - accuracy: 0.2098 - val_loss: 2.0426 - val_accuracy: 0.2110\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 817us/step - loss: 2.0293 - accuracy: 0.2113 - val_loss: 2.0255 - val_accuracy: 0.2114\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 821us/step - loss: 2.0129 - accuracy: 0.2118 - val_loss: 2.0101 - val_accuracy: 0.2125\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 769us/step - loss: 1.9981 - accuracy: 0.2130 - val_loss: 1.9964 - val_accuracy: 0.2152\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 770us/step - loss: 1.9849 - accuracy: 0.2147 - val_loss: 1.9849 - val_accuracy: 0.2176\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 835us/step - loss: 1.9729 - accuracy: 0.2170 - val_loss: 1.9733 - val_accuracy: 0.2189\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 881us/step - loss: 1.9619 - accuracy: 0.2205 - val_loss: 1.9631 - val_accuracy: 0.2186\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 795us/step - loss: 1.9517 - accuracy: 0.2226 - val_loss: 1.9531 - val_accuracy: 0.2213\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 780us/step - loss: 1.9420 - accuracy: 0.2242 - val_loss: 1.9444 - val_accuracy: 0.2265\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 860us/step - loss: 1.9323 - accuracy: 0.2253 - val_loss: 1.9346 - val_accuracy: 0.2278\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 821us/step - loss: 1.9233 - accuracy: 0.2303 - val_loss: 1.9274 - val_accuracy: 0.2297\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 782us/step - loss: 1.9144 - accuracy: 0.2323 - val_loss: 1.9184 - val_accuracy: 0.2313\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 761us/step - loss: 1.9056 - accuracy: 0.2365 - val_loss: 1.9104 - val_accuracy: 0.2356\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 726us/step - loss: 1.8972 - accuracy: 0.2419 - val_loss: 1.9024 - val_accuracy: 0.2394\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 737us/step - loss: 1.8884 - accuracy: 0.2465 - val_loss: 1.8922 - val_accuracy: 0.2490\n",
      "Epoch 54/100\n",
      "744/744 [==============================] - 1s 722us/step - loss: 1.8793 - accuracy: 0.2527 - val_loss: 1.8835 - val_accuracy: 0.2471\n",
      "Epoch 55/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 1.8697 - accuracy: 0.2573 - val_loss: 1.8773 - val_accuracy: 0.2587\n",
      "Epoch 56/100\n",
      "744/744 [==============================] - 1s 763us/step - loss: 1.8599 - accuracy: 0.2663 - val_loss: 1.8639 - val_accuracy: 0.2659\n",
      "Epoch 57/100\n",
      "744/744 [==============================] - 1s 759us/step - loss: 1.8490 - accuracy: 0.2728 - val_loss: 1.8550 - val_accuracy: 0.2648\n",
      "Epoch 58/100\n",
      "744/744 [==============================] - 1s 748us/step - loss: 1.8381 - accuracy: 0.2768 - val_loss: 1.8407 - val_accuracy: 0.2768\n",
      "Epoch 59/100\n",
      "744/744 [==============================] - 1s 749us/step - loss: 1.8261 - accuracy: 0.2851 - val_loss: 1.8287 - val_accuracy: 0.2817\n",
      "Epoch 60/100\n",
      "744/744 [==============================] - 1s 751us/step - loss: 1.8138 - accuracy: 0.2905 - val_loss: 1.8165 - val_accuracy: 0.2895\n",
      "Epoch 61/100\n",
      "744/744 [==============================] - 1s 735us/step - loss: 1.8007 - accuracy: 0.2982 - val_loss: 1.8104 - val_accuracy: 0.2973\n",
      "Epoch 62/100\n",
      "744/744 [==============================] - 1s 735us/step - loss: 1.7866 - accuracy: 0.3037 - val_loss: 1.7871 - val_accuracy: 0.3070\n",
      "Epoch 63/100\n",
      "744/744 [==============================] - 1s 725us/step - loss: 1.7714 - accuracy: 0.3082 - val_loss: 1.7706 - val_accuracy: 0.3117\n",
      "Epoch 64/100\n",
      "744/744 [==============================] - 1s 766us/step - loss: 1.7544 - accuracy: 0.3147 - val_loss: 1.7538 - val_accuracy: 0.3333\n",
      "Epoch 65/100\n",
      "744/744 [==============================] - 1s 767us/step - loss: 1.7366 - accuracy: 0.3449 - val_loss: 1.7394 - val_accuracy: 0.3465\n",
      "Epoch 66/100\n",
      "744/744 [==============================] - 1s 751us/step - loss: 1.7172 - accuracy: 0.3489 - val_loss: 1.7155 - val_accuracy: 0.3379\n",
      "Epoch 67/100\n",
      "744/744 [==============================] - 1s 766us/step - loss: 1.6957 - accuracy: 0.3396 - val_loss: 1.6933 - val_accuracy: 0.3340\n",
      "Epoch 68/100\n",
      "744/744 [==============================] - 1s 851us/step - loss: 1.6725 - accuracy: 0.3333 - val_loss: 1.7039 - val_accuracy: 0.2914\n",
      "Epoch 69/100\n",
      "744/744 [==============================] - 1s 882us/step - loss: 1.6492 - accuracy: 0.3313 - val_loss: 1.6482 - val_accuracy: 0.3305\n",
      "Epoch 70/100\n",
      "744/744 [==============================] - 1s 915us/step - loss: 1.6249 - accuracy: 0.3416 - val_loss: 1.6289 - val_accuracy: 0.3454\n",
      "Epoch 71/100\n",
      "744/744 [==============================] - 1s 856us/step - loss: 1.6007 - accuracy: 0.3467 - val_loss: 1.6023 - val_accuracy: 0.3444\n",
      "Epoch 72/100\n",
      "744/744 [==============================] - 1s 728us/step - loss: 1.5766 - accuracy: 0.3583 - val_loss: 1.5781 - val_accuracy: 0.3708\n",
      "Epoch 73/100\n",
      "744/744 [==============================] - 1s 736us/step - loss: 1.5529 - accuracy: 0.3666 - val_loss: 1.5556 - val_accuracy: 0.3740\n",
      "Epoch 74/100\n",
      "744/744 [==============================] - 1s 724us/step - loss: 1.5306 - accuracy: 0.3760 - val_loss: 1.5344 - val_accuracy: 0.3816\n",
      "Epoch 75/100\n",
      "744/744 [==============================] - 1s 734us/step - loss: 1.5100 - accuracy: 0.3900 - val_loss: 1.5108 - val_accuracy: 0.4075\n",
      "Epoch 76/100\n",
      "744/744 [==============================] - 1s 726us/step - loss: 1.4910 - accuracy: 0.4009 - val_loss: 1.4941 - val_accuracy: 0.4095\n",
      "Epoch 77/100\n",
      "744/744 [==============================] - 1s 722us/step - loss: 1.4724 - accuracy: 0.4099 - val_loss: 1.4754 - val_accuracy: 0.4254\n",
      "Epoch 78/100\n",
      "744/744 [==============================] - 1s 748us/step - loss: 1.4564 - accuracy: 0.4214 - val_loss: 1.4558 - val_accuracy: 0.4263\n",
      "Epoch 79/100\n",
      "744/744 [==============================] - 1s 737us/step - loss: 1.4407 - accuracy: 0.4289 - val_loss: 1.4421 - val_accuracy: 0.4300\n",
      "Epoch 80/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 1.4244 - accuracy: 0.4383 - val_loss: 1.4609 - val_accuracy: 0.4489\n",
      "Epoch 81/100\n",
      "744/744 [==============================] - 1s 803us/step - loss: 1.4094 - accuracy: 0.4495 - val_loss: 1.4497 - val_accuracy: 0.4487\n",
      "Epoch 82/100\n",
      "744/744 [==============================] - 1s 818us/step - loss: 1.3980 - accuracy: 0.4535 - val_loss: 1.4118 - val_accuracy: 0.4271\n",
      "Epoch 83/100\n",
      "744/744 [==============================] - 1s 907us/step - loss: 1.3837 - accuracy: 0.4643 - val_loss: 1.3913 - val_accuracy: 0.4732\n",
      "Epoch 84/100\n",
      "744/744 [==============================] - 1s 891us/step - loss: 1.3676 - accuracy: 0.4763 - val_loss: 1.3686 - val_accuracy: 0.4727\n",
      "Epoch 85/100\n",
      "744/744 [==============================] - 1s 761us/step - loss: 1.3582 - accuracy: 0.4811 - val_loss: 1.3695 - val_accuracy: 0.4576\n",
      "Epoch 86/100\n",
      "744/744 [==============================] - 1s 787us/step - loss: 1.3423 - accuracy: 0.4953 - val_loss: 1.3385 - val_accuracy: 0.5073\n",
      "Epoch 87/100\n",
      "744/744 [==============================] - 1s 747us/step - loss: 1.3271 - accuracy: 0.5044 - val_loss: 1.3540 - val_accuracy: 0.5097\n",
      "Epoch 88/100\n",
      "744/744 [==============================] - 1s 780us/step - loss: 1.3124 - accuracy: 0.5169 - val_loss: 1.4690 - val_accuracy: 0.4079\n",
      "Epoch 89/100\n",
      "744/744 [==============================] - 1s 765us/step - loss: 1.2963 - accuracy: 0.5257 - val_loss: 1.3062 - val_accuracy: 0.5311\n",
      "Epoch 90/100\n",
      "744/744 [==============================] - 1s 743us/step - loss: 1.2787 - accuracy: 0.5431 - val_loss: 1.3049 - val_accuracy: 0.5176\n",
      "Epoch 91/100\n",
      "744/744 [==============================] - 1s 788us/step - loss: 1.2600 - accuracy: 0.5611 - val_loss: 1.2602 - val_accuracy: 0.5832\n",
      "Epoch 92/100\n",
      "744/744 [==============================] - 1s 765us/step - loss: 1.2511 - accuracy: 0.5656 - val_loss: 1.2460 - val_accuracy: 0.5844\n",
      "Epoch 93/100\n",
      "744/744 [==============================] - 1s 762us/step - loss: 1.2315 - accuracy: 0.5774 - val_loss: 1.4662 - val_accuracy: 0.3783\n",
      "Epoch 94/100\n",
      "744/744 [==============================] - 1s 741us/step - loss: 1.2189 - accuracy: 0.5844 - val_loss: 1.2591 - val_accuracy: 0.5879\n",
      "Epoch 95/100\n",
      "744/744 [==============================] - 1s 726us/step - loss: 1.1999 - accuracy: 0.5993 - val_loss: 1.2673 - val_accuracy: 0.5781\n",
      "Epoch 96/100\n",
      "744/744 [==============================] - 1s 706us/step - loss: 1.1925 - accuracy: 0.6035 - val_loss: 1.1917 - val_accuracy: 0.6229\n",
      "Epoch 97/100\n",
      "744/744 [==============================] - 1s 725us/step - loss: 1.1774 - accuracy: 0.6160 - val_loss: 1.2230 - val_accuracy: 0.5819\n",
      "Epoch 98/100\n",
      "744/744 [==============================] - 1s 716us/step - loss: 1.1624 - accuracy: 0.6207 - val_loss: 1.1726 - val_accuracy: 0.6241\n",
      "Epoch 99/100\n",
      "744/744 [==============================] - 1s 745us/step - loss: 1.1530 - accuracy: 0.6250 - val_loss: 1.1650 - val_accuracy: 0.6413\n",
      "Epoch 100/100\n",
      "744/744 [==============================] - 1s 697us/step - loss: 1.1429 - accuracy: 0.6270 - val_loss: 1.1566 - val_accuracy: 0.6410\n",
      "372/372 [==============================] - 0s 434us/step - loss: 1.1468 - accuracy: 0.6394\n",
      "744/744 [==============================] - 0s 467us/step - loss: 1.1087 - accuracy: 0.6576\n",
      "Epoch 1/100\n",
      "   2/1116 [..............................] - ETA: 2:56 - loss: 2.3027 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0005s vs `on_train_batch_end` time: 0.3155s). Check your callbacks.\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 2.3024 - accuracy: 0.0922 - val_loss: 2.3022 - val_accuracy: 0.1116\n",
      "Epoch 2/100\n",
      "1116/1116 [==============================] - 1s 682us/step - loss: 2.3019 - accuracy: 0.1115 - val_loss: 2.3017 - val_accuracy: 0.1116\n",
      "Epoch 3/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 2.3015 - accuracy: 0.1115 - val_loss: 2.3013 - val_accuracy: 0.1116\n",
      "Epoch 4/100\n",
      "1116/1116 [==============================] - 1s 680us/step - loss: 2.3012 - accuracy: 0.1115 - val_loss: 2.3010 - val_accuracy: 0.1116\n",
      "Epoch 5/100\n",
      "1116/1116 [==============================] - 1s 685us/step - loss: 2.3009 - accuracy: 0.1115 - val_loss: 2.3007 - val_accuracy: 0.1116\n",
      "Epoch 6/100\n",
      "1116/1116 [==============================] - 1s 707us/step - loss: 2.3006 - accuracy: 0.1115 - val_loss: 2.3004 - val_accuracy: 0.1116\n",
      "Epoch 7/100\n",
      "1116/1116 [==============================] - 1s 672us/step - loss: 2.3003 - accuracy: 0.1115 - val_loss: 2.3001 - val_accuracy: 0.1116\n",
      "Epoch 8/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 2.3000 - accuracy: 0.1115 - val_loss: 2.2999 - val_accuracy: 0.1116\n",
      "Epoch 9/100\n",
      "1116/1116 [==============================] - 1s 683us/step - loss: 2.2997 - accuracy: 0.1115 - val_loss: 2.2996 - val_accuracy: 0.1116\n",
      "Epoch 10/100\n",
      "1116/1116 [==============================] - 1s 689us/step - loss: 2.2994 - accuracy: 0.1115 - val_loss: 2.2992 - val_accuracy: 0.1116\n",
      "Epoch 11/100\n",
      "1116/1116 [==============================] - 1s 684us/step - loss: 2.2990 - accuracy: 0.1115 - val_loss: 2.2988 - val_accuracy: 0.1116\n",
      "Epoch 12/100\n",
      "1116/1116 [==============================] - 1s 686us/step - loss: 2.2986 - accuracy: 0.1115 - val_loss: 2.2983 - val_accuracy: 0.1116\n",
      "Epoch 13/100\n",
      "1116/1116 [==============================] - 1s 704us/step - loss: 2.2981 - accuracy: 0.1115 - val_loss: 2.2978 - val_accuracy: 0.1116\n",
      "Epoch 14/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 2.2974 - accuracy: 0.1115 - val_loss: 2.2971 - val_accuracy: 0.1116\n",
      "Epoch 15/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 2.2967 - accuracy: 0.1115 - val_loss: 2.2963 - val_accuracy: 0.1116\n",
      "Epoch 16/100\n",
      "1116/1116 [==============================] - 1s 678us/step - loss: 2.2958 - accuracy: 0.1115 - val_loss: 2.2953 - val_accuracy: 0.1116\n",
      "Epoch 17/100\n",
      "1116/1116 [==============================] - 1s 682us/step - loss: 2.2947 - accuracy: 0.1115 - val_loss: 2.2941 - val_accuracy: 0.1116\n",
      "Epoch 18/100\n",
      "1116/1116 [==============================] - 1s 687us/step - loss: 2.2933 - accuracy: 0.1115 - val_loss: 2.2925 - val_accuracy: 0.1116\n",
      "Epoch 19/100\n",
      "1116/1116 [==============================] - 1s 687us/step - loss: 2.2915 - accuracy: 0.1115 - val_loss: 2.2905 - val_accuracy: 0.1116\n",
      "Epoch 20/100\n",
      "1116/1116 [==============================] - 1s 707us/step - loss: 2.2891 - accuracy: 0.1115 - val_loss: 2.2878 - val_accuracy: 0.1146\n",
      "Epoch 21/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 2.2859 - accuracy: 0.1346 - val_loss: 2.2840 - val_accuracy: 0.1476\n",
      "Epoch 22/100\n",
      "1116/1116 [==============================] - 1s 686us/step - loss: 2.2815 - accuracy: 0.1557 - val_loss: 2.2789 - val_accuracy: 0.1649\n",
      "Epoch 23/100\n",
      "1116/1116 [==============================] - 1s 694us/step - loss: 2.2753 - accuracy: 0.1713 - val_loss: 2.2716 - val_accuracy: 0.1702\n",
      "Epoch 24/100\n",
      "1116/1116 [==============================] - 1s 678us/step - loss: 2.2665 - accuracy: 0.1790 - val_loss: 2.2612 - val_accuracy: 0.1806\n",
      "Epoch 25/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 2.2540 - accuracy: 0.1850 - val_loss: 2.2465 - val_accuracy: 0.1881\n",
      "Epoch 26/100\n",
      "1116/1116 [==============================] - 1s 676us/step - loss: 2.2363 - accuracy: 0.1887 - val_loss: 2.2255 - val_accuracy: 0.1862\n",
      "Epoch 27/100\n",
      "1116/1116 [==============================] - 1s 691us/step - loss: 2.2115 - accuracy: 0.1785 - val_loss: 2.1967 - val_accuracy: 0.1705\n",
      "Epoch 28/100\n",
      "1116/1116 [==============================] - 1s 684us/step - loss: 2.1789 - accuracy: 0.1708 - val_loss: 2.1600 - val_accuracy: 0.1670\n",
      "Epoch 29/100\n",
      "1116/1116 [==============================] - 1s 675us/step - loss: 2.1391 - accuracy: 0.1703 - val_loss: 2.1171 - val_accuracy: 0.1686\n",
      "Epoch 30/100\n",
      "1116/1116 [==============================] - 1s 674us/step - loss: 2.0949 - accuracy: 0.1773 - val_loss: 2.0712 - val_accuracy: 0.1843\n",
      "Epoch 31/100\n",
      "1116/1116 [==============================] - 1s 670us/step - loss: 2.0496 - accuracy: 0.2062 - val_loss: 2.0258 - val_accuracy: 0.2079\n",
      "Epoch 32/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 2.0048 - accuracy: 0.2096 - val_loss: 1.9814 - val_accuracy: 0.2094\n",
      "Epoch 33/100\n",
      "1116/1116 [==============================] - 1s 693us/step - loss: 1.9611 - accuracy: 0.2107 - val_loss: 1.9383 - val_accuracy: 0.2114\n",
      "Epoch 34/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 1.9191 - accuracy: 0.2131 - val_loss: 1.8980 - val_accuracy: 0.2171\n",
      "Epoch 35/100\n",
      "1116/1116 [==============================] - 1s 713us/step - loss: 1.8807 - accuracy: 0.2180 - val_loss: 1.8642 - val_accuracy: 0.2224\n",
      "Epoch 36/100\n",
      "1116/1116 [==============================] - 1s 684us/step - loss: 1.8476 - accuracy: 0.2178 - val_loss: 1.8326 - val_accuracy: 0.2190\n",
      "Epoch 37/100\n",
      "1116/1116 [==============================] - 1s 675us/step - loss: 1.8191 - accuracy: 0.2324 - val_loss: 1.8066 - val_accuracy: 0.2405\n",
      "Epoch 38/100\n",
      "1116/1116 [==============================] - 1s 684us/step - loss: 1.7934 - accuracy: 0.2505 - val_loss: 1.7826 - val_accuracy: 0.2563\n",
      "Epoch 39/100\n",
      "1116/1116 [==============================] - 1s 679us/step - loss: 1.7700 - accuracy: 0.2641 - val_loss: 1.7576 - val_accuracy: 0.2673\n",
      "Epoch 40/100\n",
      "1116/1116 [==============================] - 1s 682us/step - loss: 1.7486 - accuracy: 0.2738 - val_loss: 1.7397 - val_accuracy: 0.2710\n",
      "Epoch 41/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 1.7291 - accuracy: 0.2836 - val_loss: 1.7171 - val_accuracy: 0.2848\n",
      "Epoch 42/100\n",
      "1116/1116 [==============================] - 1s 699us/step - loss: 1.7105 - accuracy: 0.2934 - val_loss: 1.7032 - val_accuracy: 0.2914\n",
      "Epoch 43/100\n",
      "1116/1116 [==============================] - 1s 699us/step - loss: 1.6937 - accuracy: 0.3036 - val_loss: 1.6906 - val_accuracy: 0.3049\n",
      "Epoch 44/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 1.6778 - accuracy: 0.3123 - val_loss: 1.6683 - val_accuracy: 0.3181\n",
      "Epoch 45/100\n",
      "1116/1116 [==============================] - 1s 713us/step - loss: 1.6621 - accuracy: 0.3197 - val_loss: 1.6525 - val_accuracy: 0.3232\n",
      "Epoch 46/100\n",
      "1116/1116 [==============================] - 1s 684us/step - loss: 1.6451 - accuracy: 0.3282 - val_loss: 1.6378 - val_accuracy: 0.3352\n",
      "Epoch 47/100\n",
      "1116/1116 [==============================] - 1s 709us/step - loss: 1.6268 - accuracy: 0.3383 - val_loss: 1.6213 - val_accuracy: 0.3405\n",
      "Epoch 48/100\n",
      "1116/1116 [==============================] - 1s 690us/step - loss: 1.6072 - accuracy: 0.3484 - val_loss: 1.5972 - val_accuracy: 0.3581\n",
      "Epoch 49/100\n",
      "1116/1116 [==============================] - 1s 704us/step - loss: 1.5842 - accuracy: 0.3653 - val_loss: 1.5771 - val_accuracy: 0.3694\n",
      "Epoch 50/100\n",
      "1116/1116 [==============================] - 1s 685us/step - loss: 1.5614 - accuracy: 0.3796 - val_loss: 1.5516 - val_accuracy: 0.3822\n",
      "Epoch 51/100\n",
      "1116/1116 [==============================] - 1s 697us/step - loss: 1.5366 - accuracy: 0.3951 - val_loss: 1.5429 - val_accuracy: 0.4005\n",
      "Epoch 52/100\n",
      "1116/1116 [==============================] - 1s 678us/step - loss: 1.5113 - accuracy: 0.4075 - val_loss: 1.5065 - val_accuracy: 0.4211\n",
      "Epoch 53/100\n",
      "1116/1116 [==============================] - 1s 678us/step - loss: 1.4877 - accuracy: 0.4213 - val_loss: 1.4810 - val_accuracy: 0.4325\n",
      "Epoch 54/100\n",
      "1116/1116 [==============================] - 1s 676us/step - loss: 1.4648 - accuracy: 0.4326 - val_loss: 1.4683 - val_accuracy: 0.4443\n",
      "Epoch 55/100\n",
      "1116/1116 [==============================] - 1s 690us/step - loss: 1.4454 - accuracy: 0.4414 - val_loss: 1.4440 - val_accuracy: 0.4371\n",
      "Epoch 56/100\n",
      "1116/1116 [==============================] - 1s 704us/step - loss: 1.4273 - accuracy: 0.4464 - val_loss: 1.4244 - val_accuracy: 0.4543\n",
      "Epoch 57/100\n",
      "1116/1116 [==============================] - 1s 687us/step - loss: 1.4113 - accuracy: 0.4554 - val_loss: 1.5030 - val_accuracy: 0.3916\n",
      "Epoch 58/100\n",
      "1116/1116 [==============================] - 1s 672us/step - loss: 1.3963 - accuracy: 0.4593 - val_loss: 1.3927 - val_accuracy: 0.4671\n",
      "Epoch 59/100\n",
      "1116/1116 [==============================] - 1s 680us/step - loss: 1.3848 - accuracy: 0.4641 - val_loss: 1.3884 - val_accuracy: 0.4727\n",
      "Epoch 60/100\n",
      "1116/1116 [==============================] - 1s 683us/step - loss: 1.3710 - accuracy: 0.4709 - val_loss: 1.3676 - val_accuracy: 0.4810\n",
      "Epoch 61/100\n",
      "1116/1116 [==============================] - 1s 690us/step - loss: 1.3570 - accuracy: 0.4761 - val_loss: 1.5078 - val_accuracy: 0.4033\n",
      "Epoch 62/100\n",
      "1116/1116 [==============================] - 1s 680us/step - loss: 1.3423 - accuracy: 0.4825 - val_loss: 1.3890 - val_accuracy: 0.4562\n",
      "Epoch 63/100\n",
      "1116/1116 [==============================] - 1s 684us/step - loss: 1.3308 - accuracy: 0.4892 - val_loss: 1.3287 - val_accuracy: 0.4911\n",
      "Epoch 64/100\n",
      "1116/1116 [==============================] - 1s 693us/step - loss: 1.3197 - accuracy: 0.4947 - val_loss: 1.3182 - val_accuracy: 0.4879\n",
      "Epoch 65/100\n",
      "1116/1116 [==============================] - 1s 705us/step - loss: 1.3100 - accuracy: 0.4947 - val_loss: 1.3167 - val_accuracy: 0.5041\n",
      "Epoch 66/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 1.2992 - accuracy: 0.5025 - val_loss: 1.2941 - val_accuracy: 0.5078\n",
      "Epoch 67/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 1.2853 - accuracy: 0.5084 - val_loss: 1.3269 - val_accuracy: 0.4856\n",
      "Epoch 68/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 1.2790 - accuracy: 0.5076 - val_loss: 1.2766 - val_accuracy: 0.5105\n",
      "Epoch 69/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 1.2661 - accuracy: 0.5110 - val_loss: 1.3177 - val_accuracy: 0.4819\n",
      "Epoch 70/100\n",
      "1116/1116 [==============================] - 1s 672us/step - loss: 1.2606 - accuracy: 0.5136 - val_loss: 1.3679 - val_accuracy: 0.4544\n",
      "Epoch 71/100\n",
      "1116/1116 [==============================] - 1s 684us/step - loss: 1.2518 - accuracy: 0.5158 - val_loss: 1.2808 - val_accuracy: 0.4933\n",
      "Epoch 72/100\n",
      "1116/1116 [==============================] - 1s 677us/step - loss: 1.2429 - accuracy: 0.5185 - val_loss: 1.2548 - val_accuracy: 0.5356\n",
      "Epoch 73/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 1.2343 - accuracy: 0.5225 - val_loss: 1.2688 - val_accuracy: 0.5071\n",
      "Epoch 74/100\n",
      "1116/1116 [==============================] - 1s 676us/step - loss: 1.2288 - accuracy: 0.5237 - val_loss: 1.2334 - val_accuracy: 0.5308\n",
      "Epoch 75/100\n",
      "1116/1116 [==============================] - 1s 685us/step - loss: 1.2169 - accuracy: 0.5337 - val_loss: 1.2310 - val_accuracy: 0.5256\n",
      "Epoch 76/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 1.2085 - accuracy: 0.5339 - val_loss: 1.2400 - val_accuracy: 0.5230\n",
      "Epoch 77/100\n",
      "1116/1116 [==============================] - 1s 693us/step - loss: 1.1987 - accuracy: 0.5413 - val_loss: 1.2132 - val_accuracy: 0.5357\n",
      "Epoch 78/100\n",
      "1116/1116 [==============================] - 1s 695us/step - loss: 1.1880 - accuracy: 0.5545 - val_loss: 1.2632 - val_accuracy: 0.5259\n",
      "Epoch 79/100\n",
      "1116/1116 [==============================] - 1s 694us/step - loss: 1.1785 - accuracy: 0.5593 - val_loss: 1.1845 - val_accuracy: 0.5746\n",
      "Epoch 80/100\n",
      "1116/1116 [==============================] - 1s 693us/step - loss: 1.1659 - accuracy: 0.5758 - val_loss: 1.1714 - val_accuracy: 0.5765\n",
      "Epoch 81/100\n",
      "1116/1116 [==============================] - 1s 691us/step - loss: 1.1539 - accuracy: 0.5890 - val_loss: 1.1849 - val_accuracy: 0.5852\n",
      "Epoch 82/100\n",
      "1116/1116 [==============================] - 1s 682us/step - loss: 1.1412 - accuracy: 0.6062 - val_loss: 1.1662 - val_accuracy: 0.6075\n",
      "Epoch 83/100\n",
      "1116/1116 [==============================] - 1s 692us/step - loss: 1.1257 - accuracy: 0.6150 - val_loss: 1.1862 - val_accuracy: 0.5916\n",
      "Epoch 84/100\n",
      "1116/1116 [==============================] - 1s 693us/step - loss: 1.1122 - accuracy: 0.6289 - val_loss: 1.1231 - val_accuracy: 0.6232\n",
      "Epoch 85/100\n",
      "1116/1116 [==============================] - 1s 705us/step - loss: 1.0920 - accuracy: 0.6412 - val_loss: 1.1403 - val_accuracy: 0.6184\n",
      "Epoch 86/100\n",
      "1116/1116 [==============================] - 1s 692us/step - loss: 1.0685 - accuracy: 0.6545 - val_loss: 1.0749 - val_accuracy: 0.6527\n",
      "Epoch 87/100\n",
      "1116/1116 [==============================] - 1s 689us/step - loss: 1.0451 - accuracy: 0.6659 - val_loss: 1.0420 - val_accuracy: 0.6719\n",
      "Epoch 88/100\n",
      "1116/1116 [==============================] - 1s 681us/step - loss: 1.0186 - accuracy: 0.6814 - val_loss: 1.0208 - val_accuracy: 0.6792\n",
      "Epoch 89/100\n",
      "1116/1116 [==============================] - 1s 679us/step - loss: 0.9913 - accuracy: 0.6951 - val_loss: 0.9842 - val_accuracy: 0.7052\n",
      "Epoch 90/100\n",
      "1116/1116 [==============================] - 1s 694us/step - loss: 0.9589 - accuracy: 0.7124 - val_loss: 0.9684 - val_accuracy: 0.7068\n",
      "Epoch 91/100\n",
      "1116/1116 [==============================] - 1s 683us/step - loss: 0.9288 - accuracy: 0.7259 - val_loss: 0.9276 - val_accuracy: 0.7295\n",
      "Epoch 92/100\n",
      "1116/1116 [==============================] - 1s 716us/step - loss: 0.9008 - accuracy: 0.7385 - val_loss: 1.3874 - val_accuracy: 0.5357\n",
      "Epoch 93/100\n",
      "1116/1116 [==============================] - 1s 706us/step - loss: 0.8721 - accuracy: 0.7493 - val_loss: 0.8874 - val_accuracy: 0.7495\n",
      "Epoch 94/100\n",
      "1116/1116 [==============================] - 1s 689us/step - loss: 0.8447 - accuracy: 0.7578 - val_loss: 0.9401 - val_accuracy: 0.6976\n",
      "Epoch 95/100\n",
      "1116/1116 [==============================] - 1s 701us/step - loss: 0.8198 - accuracy: 0.7637 - val_loss: 0.8211 - val_accuracy: 0.7610\n",
      "Epoch 96/100\n",
      "1116/1116 [==============================] - 1s 705us/step - loss: 0.7965 - accuracy: 0.7693 - val_loss: 0.8146 - val_accuracy: 0.7675\n",
      "Epoch 97/100\n",
      "1116/1116 [==============================] - 1s 690us/step - loss: 0.7735 - accuracy: 0.7778 - val_loss: 0.8449 - val_accuracy: 0.7590\n",
      "Epoch 98/100\n",
      "1116/1116 [==============================] - 1s 729us/step - loss: 0.7557 - accuracy: 0.7845 - val_loss: 0.7641 - val_accuracy: 0.7837\n",
      "Epoch 99/100\n",
      "1116/1116 [==============================] - 1s 714us/step - loss: 0.7390 - accuracy: 0.7881 - val_loss: 0.7536 - val_accuracy: 0.7822\n",
      "Epoch 100/100\n",
      "1116/1116 [==============================] - 1s 690us/step - loss: 0.7240 - accuracy: 0.7938 - val_loss: 0.7846 - val_accuracy: 0.7668\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "source": [
    "history_ker_reg.best_params_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_neurons': 10, 'n_hidden': 10, 'learning_rate': 0.001}"
      ]
     },
     "metadata": {},
     "execution_count": 379
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "source": [
    "history_ker_reg"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000002B0E2D38370>,\n",
       "                   param_distributions={'learning_rate': [0.001],\n",
       "                                        'n_hidden': [10], 'n_neurons': [10]},\n",
       "                   random_state=43, return_train_score=True)"
      ]
     },
     "metadata": {},
     "execution_count": 380
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TK - Model Training with Full Dataset and Discovered Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "source": [
    "# Creating wrapped regression model with our function.      BACKUP: {'n_neurons': 50, 'n_hidden': 10, 'learning_rate': 0.001}\r\n",
    "keras_reg_model = keras_reg.build_fn(n_neurons= 10, n_hidden= 10, learning_rate=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "source": [
    "keras_reg_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1829 (Dense)           (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "dense_1830 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1831 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1832 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1833 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1834 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1835 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1836 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1837 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1838 (Dense)           (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1839 (Dense)           (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 8,950\n",
      "Trainable params: 8,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "source": [
    "# creating a new log dir for tensorboard\r\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\r\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_keras_reg_model.h5\", save_best_only=False, save_weights_only=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "source": [
    "# preparing data based on our beautifull trained data pipeline\r\n",
    "X_prep_all = pipeline.transform(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "source": [
    "# Test after pipeline use\r\n",
    "plt.imshow(X_prep_all[1].reshape(28,28), cmap='Greys')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0dddd9760>"
      ]
     },
     "metadata": {},
     "execution_count": 385
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-27T18:13:15.228441</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p2f44893b0b)\">\r\n    <image height=\"218\" id=\"image47483352a0\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m53e368d441\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m53e368d441\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m53e368d441\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m53e368d441\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m53e368d441\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m53e368d441\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m53e368d441\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mb276a243f4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb276a243f4\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb276a243f4\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb276a243f4\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb276a243f4\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb276a243f4\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb276a243f4\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p2f44893b0b\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \r\n",
    "keras_reg_model.fit(X_prep_all, y, epochs=100, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "   2/1313 [..............................] - ETA: 6:29 - loss: 2.3026 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.5915s). Check your callbacks.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 2.3021 - accuracy: 0.1116\n",
      "Epoch 2/100\n",
      "1313/1313 [==============================] - 1s 667us/step - loss: 2.3013 - accuracy: 0.1115\n",
      "Epoch 3/100\n",
      "1313/1313 [==============================] - 1s 643us/step - loss: 2.3007 - accuracy: 0.1115\n",
      "Epoch 4/100\n",
      "1313/1313 [==============================] - 1s 630us/step - loss: 2.3001 - accuracy: 0.1115\n",
      "Epoch 5/100\n",
      "1313/1313 [==============================] - 1s 633us/step - loss: 2.2995 - accuracy: 0.1115\n",
      "Epoch 6/100\n",
      "1313/1313 [==============================] - 1s 636us/step - loss: 2.2989 - accuracy: 0.1115\n",
      "Epoch 7/100\n",
      "1313/1313 [==============================] - 1s 621us/step - loss: 2.2981 - accuracy: 0.1115\n",
      "Epoch 8/100\n",
      "1313/1313 [==============================] - 1s 656us/step - loss: 2.2972 - accuracy: 0.1115\n",
      "Epoch 9/100\n",
      "1313/1313 [==============================] - 1s 634us/step - loss: 2.2962 - accuracy: 0.1115\n",
      "Epoch 10/100\n",
      "1313/1313 [==============================] - 1s 628us/step - loss: 2.2949 - accuracy: 0.1115\n",
      "Epoch 11/100\n",
      "1313/1313 [==============================] - 1s 636us/step - loss: 2.2933 - accuracy: 0.1115\n",
      "Epoch 12/100\n",
      "1313/1313 [==============================] - 1s 621us/step - loss: 2.2911 - accuracy: 0.1115\n",
      "Epoch 13/100\n",
      "1313/1313 [==============================] - 1s 609us/step - loss: 2.2883 - accuracy: 0.1115\n",
      "Epoch 14/100\n",
      "1313/1313 [==============================] - 1s 632us/step - loss: 2.2845 - accuracy: 0.1120\n",
      "Epoch 15/100\n",
      "1313/1313 [==============================] - 1s 653us/step - loss: 2.2795 - accuracy: 0.1413\n",
      "Epoch 16/100\n",
      "1313/1313 [==============================] - 1s 640us/step - loss: 2.2725 - accuracy: 0.1761\n",
      "Epoch 17/100\n",
      "1313/1313 [==============================] - 1s 649us/step - loss: 2.2629 - accuracy: 0.1929\n",
      "Epoch 18/100\n",
      "1313/1313 [==============================] - 1s 641us/step - loss: 2.2495 - accuracy: 0.1997\n",
      "Epoch 19/100\n",
      "1313/1313 [==============================] - 1s 643us/step - loss: 2.2312 - accuracy: 0.2026\n",
      "Epoch 20/100\n",
      "1313/1313 [==============================] - 1s 655us/step - loss: 2.2078 - accuracy: 0.2031\n",
      "Epoch 21/100\n",
      "1313/1313 [==============================] - 1s 663us/step - loss: 2.1795 - accuracy: 0.2063\n",
      "Epoch 22/100\n",
      "1313/1313 [==============================] - 1s 684us/step - loss: 2.1484 - accuracy: 0.2047\n",
      "Epoch 23/100\n",
      "1313/1313 [==============================] - 1s 650us/step - loss: 2.1168 - accuracy: 0.2067\n",
      "Epoch 24/100\n",
      "1313/1313 [==============================] - 1s 634us/step - loss: 2.0873 - accuracy: 0.2054\n",
      "Epoch 25/100\n",
      "1313/1313 [==============================] - 1s 645us/step - loss: 2.0606 - accuracy: 0.2066\n",
      "Epoch 26/100\n",
      "1313/1313 [==============================] - 1s 653us/step - loss: 2.0368 - accuracy: 0.2053\n",
      "Epoch 27/100\n",
      "1313/1313 [==============================] - 1s 660us/step - loss: 2.0150 - accuracy: 0.2056\n",
      "Epoch 28/100\n",
      "1313/1313 [==============================] - 1s 747us/step - loss: 1.9946 - accuracy: 0.2085\n",
      "Epoch 29/100\n",
      "1313/1313 [==============================] - 1s 749us/step - loss: 1.9747 - accuracy: 0.2111\n",
      "Epoch 30/100\n",
      "1313/1313 [==============================] - 1s 698us/step - loss: 1.9540 - accuracy: 0.2206\n",
      "Epoch 31/100\n",
      "1313/1313 [==============================] - 1s 663us/step - loss: 1.9318 - accuracy: 0.2282\n",
      "Epoch 32/100\n",
      "1313/1313 [==============================] - 1s 664us/step - loss: 1.9086 - accuracy: 0.2567\n",
      "Epoch 33/100\n",
      "1313/1313 [==============================] - 1s 735us/step - loss: 1.8844 - accuracy: 0.2580\n",
      "Epoch 34/100\n",
      "1313/1313 [==============================] - 1s 741us/step - loss: 1.8586 - accuracy: 0.2716\n",
      "Epoch 35/100\n",
      "1313/1313 [==============================] - 1s 714us/step - loss: 1.8317 - accuracy: 0.2578\n",
      "Epoch 36/100\n",
      "1313/1313 [==============================] - 1s 629us/step - loss: 1.8026 - accuracy: 0.2768\n",
      "Epoch 37/100\n",
      "1313/1313 [==============================] - 1s 649us/step - loss: 1.7694 - accuracy: 0.2993\n",
      "Epoch 38/100\n",
      "1313/1313 [==============================] - 1s 630us/step - loss: 1.7303 - accuracy: 0.3279\n",
      "Epoch 39/100\n",
      "1313/1313 [==============================] - 1s 660us/step - loss: 1.6832 - accuracy: 0.3542\n",
      "Epoch 40/100\n",
      "1313/1313 [==============================] - 1s 684us/step - loss: 1.6267 - accuracy: 0.3820\n",
      "Epoch 41/100\n",
      "1313/1313 [==============================] - 1s 684us/step - loss: 1.5639 - accuracy: 0.4118\n",
      "Epoch 42/100\n",
      "1313/1313 [==============================] - 1s 685us/step - loss: 1.5062 - accuracy: 0.4356\n",
      "Epoch 43/100\n",
      "1313/1313 [==============================] - 1s 663us/step - loss: 1.4579 - accuracy: 0.4510\n",
      "Epoch 44/100\n",
      "1313/1313 [==============================] - 1s 645us/step - loss: 1.4181 - accuracy: 0.4661\n",
      "Epoch 45/100\n",
      "1313/1313 [==============================] - 1s 638us/step - loss: 1.3844 - accuracy: 0.4760\n",
      "Epoch 46/100\n",
      "1313/1313 [==============================] - 1s 658us/step - loss: 1.3576 - accuracy: 0.4809\n",
      "Epoch 47/100\n",
      "1313/1313 [==============================] - 1s 636us/step - loss: 1.3301 - accuracy: 0.4888\n",
      "Epoch 48/100\n",
      "1313/1313 [==============================] - 1s 628us/step - loss: 1.3042 - accuracy: 0.4892\n",
      "Epoch 49/100\n",
      "1313/1313 [==============================] - 1s 612us/step - loss: 1.2802 - accuracy: 0.5034\n",
      "Epoch 50/100\n",
      "1313/1313 [==============================] - 1s 624us/step - loss: 1.2575 - accuracy: 0.5112\n",
      "Epoch 51/100\n",
      "1313/1313 [==============================] - 1s 620us/step - loss: 1.2361 - accuracy: 0.5244\n",
      "Epoch 52/100\n",
      "1313/1313 [==============================] - 1s 640us/step - loss: 1.2160 - accuracy: 0.5335\n",
      "Epoch 53/100\n",
      "1313/1313 [==============================] - 1s 640us/step - loss: 1.1958 - accuracy: 0.5432\n",
      "Epoch 54/100\n",
      "1313/1313 [==============================] - 1s 636us/step - loss: 1.1696 - accuracy: 0.5550\n",
      "Epoch 55/100\n",
      "1313/1313 [==============================] - 1s 624us/step - loss: 1.1487 - accuracy: 0.5654\n",
      "Epoch 56/100\n",
      "1313/1313 [==============================] - 1s 629us/step - loss: 1.1253 - accuracy: 0.5738\n",
      "Epoch 57/100\n",
      "1313/1313 [==============================] - 1s 625us/step - loss: 1.1046 - accuracy: 0.5828\n",
      "Epoch 58/100\n",
      "1313/1313 [==============================] - 1s 653us/step - loss: 1.0860 - accuracy: 0.5918\n",
      "Epoch 59/100\n",
      "1313/1313 [==============================] - 1s 671us/step - loss: 1.0639 - accuracy: 0.6020\n",
      "Epoch 60/100\n",
      "1313/1313 [==============================] - 1s 687us/step - loss: 1.0468 - accuracy: 0.6159\n",
      "Epoch 61/100\n",
      "1313/1313 [==============================] - 1s 729us/step - loss: 1.0332 - accuracy: 0.6245\n",
      "Epoch 62/100\n",
      "1313/1313 [==============================] - 1s 719us/step - loss: 1.0191 - accuracy: 0.6322\n",
      "Epoch 63/100\n",
      "1313/1313 [==============================] - 1s 699us/step - loss: 1.0056 - accuracy: 0.6404\n",
      "Epoch 64/100\n",
      "1313/1313 [==============================] - 1s 645us/step - loss: 0.9921 - accuracy: 0.6459\n",
      "Epoch 65/100\n",
      "1313/1313 [==============================] - 1s 704us/step - loss: 0.9754 - accuracy: 0.6589\n",
      "Epoch 66/100\n",
      "1313/1313 [==============================] - 1s 756us/step - loss: 0.9632 - accuracy: 0.6661\n",
      "Epoch 67/100\n",
      "1313/1313 [==============================] - 1s 721us/step - loss: 0.9456 - accuracy: 0.6777\n",
      "Epoch 68/100\n",
      "1313/1313 [==============================] - 1s 640us/step - loss: 0.9365 - accuracy: 0.6861\n",
      "Epoch 69/100\n",
      "1313/1313 [==============================] - 1s 636us/step - loss: 0.9244 - accuracy: 0.6956\n",
      "Epoch 70/100\n",
      "1313/1313 [==============================] - 1s 632us/step - loss: 0.9113 - accuracy: 0.7010\n",
      "Epoch 71/100\n",
      "1313/1313 [==============================] - 1s 630us/step - loss: 0.9012 - accuracy: 0.7117\n",
      "Epoch 72/100\n",
      "1313/1313 [==============================] - 1s 624us/step - loss: 0.8872 - accuracy: 0.7192\n",
      "Epoch 73/100\n",
      "1313/1313 [==============================] - 1s 653us/step - loss: 0.8794 - accuracy: 0.7230\n",
      "Epoch 74/100\n",
      "1313/1313 [==============================] - 1s 632us/step - loss: 0.8697 - accuracy: 0.7296\n",
      "Epoch 75/100\n",
      "1313/1313 [==============================] - 1s 690us/step - loss: 0.8557 - accuracy: 0.7370\n",
      "Epoch 76/100\n",
      "1313/1313 [==============================] - 1s 726us/step - loss: 0.8472 - accuracy: 0.7439\n",
      "Epoch 77/100\n",
      "1313/1313 [==============================] - 1s 666us/step - loss: 0.8415 - accuracy: 0.7416\n",
      "Epoch 78/100\n",
      "1313/1313 [==============================] - 1s 736us/step - loss: 0.8310 - accuracy: 0.7501\n",
      "Epoch 79/100\n",
      "1313/1313 [==============================] - 1s 695us/step - loss: 0.8261 - accuracy: 0.7500\n",
      "Epoch 80/100\n",
      "1313/1313 [==============================] - 1s 631us/step - loss: 0.8160 - accuracy: 0.7553\n",
      "Epoch 81/100\n",
      "1313/1313 [==============================] - 1s 632us/step - loss: 0.8115 - accuracy: 0.7591\n",
      "Epoch 82/100\n",
      "1313/1313 [==============================] - 1s 630us/step - loss: 0.8042 - accuracy: 0.7618\n",
      "Epoch 83/100\n",
      "1313/1313 [==============================] - 1s 633us/step - loss: 0.7931 - accuracy: 0.7690\n",
      "Epoch 84/100\n",
      "1313/1313 [==============================] - 1s 627us/step - loss: 0.7862 - accuracy: 0.7686\n",
      "Epoch 85/100\n",
      "1313/1313 [==============================] - 1s 641us/step - loss: 0.7806 - accuracy: 0.7734\n",
      "Epoch 86/100\n",
      "1313/1313 [==============================] - 1s 651us/step - loss: 0.7747 - accuracy: 0.7767\n",
      "Epoch 87/100\n",
      "1313/1313 [==============================] - 1s 625us/step - loss: 0.7681 - accuracy: 0.7815\n",
      "Epoch 88/100\n",
      "1313/1313 [==============================] - 1s 623us/step - loss: 0.7587 - accuracy: 0.7854\n",
      "Epoch 89/100\n",
      "1313/1313 [==============================] - 1s 630us/step - loss: 0.7561 - accuracy: 0.7892\n",
      "Epoch 90/100\n",
      "1313/1313 [==============================] - 1s 624us/step - loss: 0.7501 - accuracy: 0.7908\n",
      "Epoch 91/100\n",
      "1313/1313 [==============================] - 1s 636us/step - loss: 0.7434 - accuracy: 0.7964\n",
      "Epoch 92/100\n",
      "1313/1313 [==============================] - 1s 648us/step - loss: 0.7380 - accuracy: 0.7972\n",
      "Epoch 93/100\n",
      "1313/1313 [==============================] - 1s 626us/step - loss: 0.7302 - accuracy: 0.8017\n",
      "Epoch 94/100\n",
      "1313/1313 [==============================] - 1s 631us/step - loss: 0.7300 - accuracy: 0.8047\n",
      "Epoch 95/100\n",
      "1313/1313 [==============================] - 1s 640us/step - loss: 0.7213 - accuracy: 0.8086\n",
      "Epoch 96/100\n",
      "1313/1313 [==============================] - 1s 657us/step - loss: 0.7113 - accuracy: 0.8133\n",
      "Epoch 97/100\n",
      "1313/1313 [==============================] - 1s 678us/step - loss: 0.7064 - accuracy: 0.8156\n",
      "Epoch 98/100\n",
      "1313/1313 [==============================] - 1s 754us/step - loss: 0.7047 - accuracy: 0.8171\n",
      "Epoch 99/100\n",
      "1313/1313 [==============================] - 1s 681us/step - loss: 0.6967 - accuracy: 0.8208\n",
      "Epoch 100/100\n",
      "1313/1313 [==============================] - 1s 657us/step - loss: 0.6935 - accuracy: 0.8229\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b0e2315430>"
      ]
     },
     "metadata": {},
     "execution_count": 386
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Peparing Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "source": [
    "X_test_prep = pipeline.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "source": [
    "X_test_prep"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 388
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Competition File"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction of Testdata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "source": [
    "plt.imshow(X_test_prep[43].reshape(28,28), cmap='Greys')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0db532e80>"
      ]
     },
     "metadata": {},
     "execution_count": 393
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-27T18:15:33.733443</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p5edbadda93)\">\r\n    <image height=\"218\" id=\"image4c806bcc09\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAFlElEQVR4nO3dT4jMfxzH8Z2fn2SRUpS2iJKS0qa4iHJRbsTJQUopbY4ue3JVchBxkLJu5OKguClFObgoUXubg5Lyrwkxv7PyfX9/dsxrd+zjcX312fm27bNvzbeZ7fT7/f4YMFT/zPcFwGIgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBwL/zfQHkffr0qXHbtm1befbHjx/l/vLly3JfuXJluf+t3NEgQGgQIDQIEBoECA0ChAYB3t5fhHq9XuPW7XbLs23fTlj97LExb+8DQyQ0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDvdeQnbd/b2Lbza+5oECA0CBAaBAgNAoQGAUKDAKFBgOdo/KTT6cz3JfyV3NEgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgGL9mMy9+/fL/fZ2dlyX79+/Zz3DRs2lGcnJibKndHjjgYBQoMAoUGA0CBAaBAgNAgQGgSM9HO058+fN26XL18uz16/fr3ch/m1a8uXLy/3VatWlfvVq1fLff/+/b99TQyXOxoECA0ChAYBQoMAoUGA0CBAaBAw0s/RLl261LjduHGjPLt79+5yv3XrVrk/ePCg3MfHxxu36enp8my32y33Q4cOlfu6devK/ejRo41bv98vz7bt/Jo7GgQIDQKEBgFCgwChQYDQIKDTX8Dv17a9zb1169bGrdfrlWefPXtW7pOTk+U+TK9fvy73ixcvlvu1a9f+5OX8pO3P5c2bN+W+du3aP3k5I8MdDQKEBgFCgwChQYDQIEBoECA0CFjQH5N5+/ZtuX/58mXOP3vNmjVzPjtsW7ZsKfcLFy6U+4EDB8r98OHDv31NDMYdDQKEBgFCgwChQYDQIEBoECA0CFjQz9F27NhR7nv27GncHj16VJ6dmpoq93v37pX7fGr7t08TExOhK+H/ckeDAKFBgNAgQGgQIDQIEBoECA0CFvRztDbHjh1r3Nqeo7148aLc3717V+4L+fNsmzZtKvcVK1Y0bp8/fx7otW/fvl3up0+fHujnjyp3NAgQGgQIDQKEBgFCgwChQYDQIGBB/3+0Nt++fWvc9u7dW5598uRJue/atavcnz59Wu7z6f379+W+cePGxu3Dhw/l2bY/l1H+vQ2TOxoECA0ChAYBQoMAoUGA0CBgpD8ms3Tp0sbtypUr5dmdO3eW++zsbLn3er1yb/tKuGFqu/bq2j5+/DjQa7969arcq0cPq1evHui1FzJ3NAgQGgQIDQKEBgFCgwChQYDQIGCkn6NVJicny/3x48flvm/fvnLfvn17uR8/frxxm56eLs8uWbKk3NtUH4MZGxvsK+XOnz9f7tVX2Y2N/d3PyiruaBAgNAgQGgQIDQKEBgFCgwChQcBIf93cMN28ebPcT5w4Meefffbs2XI/cuRIuX/9+rXcz507V+4PHz4s98r379/nfHYxc0eDAKFBgNAgQGgQIDQIEBoECA0CPEdr0PZrmZmZKfeTJ082bm3Potpeu9PplHubZcuWNW537twpzx48eHCg116s3NEgQGgQIDQIEBoECA0ChAYBQoMAz9GG5O7du43bmTNnyrPdbrfcx8fHy31qaqrcT5061bht3ry5PMvcuKNBgNAgQGgQIDQIEBoECA0CvL0PAe5oECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCg4D/AFP07GtzN6JmAAAAAElFTkSuQmCC\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m8233b0c54f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m8233b0c54f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m8233b0c54f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m8233b0c54f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m8233b0c54f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m8233b0c54f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m8233b0c54f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mcb09f53945\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcb09f53945\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcb09f53945\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcb09f53945\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcb09f53945\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcb09f53945\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcb09f53945\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p5edbadda93\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "source": [
    "print(\"Propability of all lables for given pixels: \", keras_reg_model.predict(X_test_prep[43].reshape(1,-1)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Propability of all lables for given pixels:  [[4.4975845e-06 4.2070023e-09 1.7020870e-04 1.0725735e-02 3.9906695e-01\n",
      "  2.7450286e-03 2.2626680e-03 2.1788647e-02 1.0249803e-02 5.5298644e-01]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "source": [
    "print(\"Predicted Digit: \",np.argmax(keras_reg_model.predict(X_test_prep[43].reshape(1,-1))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted Digit:  9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "source": [
    "i = 1\r\n",
    "for row in X_test_prep:\r\n",
    "    index = i\r\n",
    "    predicted_label = np.argmax(keras_reg_model.predict(row.reshape(1,-1)))\r\n",
    "\r\n",
    "    mnist_competition_file = mnist_competition_file.append({'ImageId': index, 'Label': predicted_label}, ignore_index = True )\r\n",
    "    i = i + 1\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "source": [
    "mnist_competition_file"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      ImageId Label\n",
       "0           1     2\n",
       "1           2     0\n",
       "2           3     9\n",
       "3           4     9\n",
       "4           5     3\n",
       "...       ...   ...\n",
       "28105   27996     9\n",
       "28106   27997     7\n",
       "28107   27998     3\n",
       "28108   27999     9\n",
       "28109   28000     2\n",
       "\n",
       "[28110 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28105</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28106</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28107</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28108</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28109</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28110 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 399
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\r\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('wingpuflake_keras': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}