{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14cc659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "#import seaborn as sn\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "from random import seed\r\n",
    "seed(1)\r\n",
    "seed = 43\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "print(\"Tensorflow Version: \", tf.__version__)\r\n",
    "print(\"Keras Version: \",keras.__version__)\r\n",
    "\r\n",
    "\r\n",
    "kaggle = 0 # Kaggle path active = 1\r\n",
    "\r\n",
    "# change your local path here\r\n",
    "if kaggle == 1 :\r\n",
    "    MNIST_PATH= '../input/digit-recognizer'\r\n",
    "else:\r\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "import os\r\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \r\n",
    "    for filename in filenames:\r\n",
    "        print(os.path.join(dirname, filename))\r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90873b55",
   "metadata": {},
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "Link to the topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "This is another Notebook to take a look into annother algorithm. Here I want to give the Deep Neural Network with the Framework Keras a try. As already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Another MNIST Try: https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters: https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7729c4",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9e2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path and file\r\n",
    "#MNIST_PATH= '../input/digit-recognizer'\r\n",
    "#MNIST_PATH= '../Another_MNIST_try/data/input/digit-recognizer'\r\n",
    "CSV_FILE_TRAIN='train.csv'\r\n",
    "CSV_FILE_TEST='test.csv'\r\n",
    "\r\n",
    "def load_mnist_data(minist_path, csv_file):\r\n",
    "    csv_path = os.path.join(minist_path, csv_file)\r\n",
    "    return pd.read_csv(csv_path)\r\n",
    "\r\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\r\n",
    "    csv_path = os.path.join(minist_path, csv_file)\r\n",
    "    csv_file = open(csv_path, 'r')\r\n",
    "    csv_data = csv_file.readlines()\r\n",
    "    csv_file.close()\r\n",
    "    return csv_data\r\n",
    "\r\n",
    "def split_train_val(data, val_ratio):\r\n",
    "    return \r\n",
    "    \r\n",
    "\r\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\r\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d349c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label'].copy()\r\n",
    "X = train.drop(['label'], axis=1)\r\n",
    "\r\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30f5f0",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57bbb6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Features: \",X.shape)\r\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72bc98",
   "metadata": {},
   "source": [
    "### TK - Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499c3f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7ea066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.15\r\n",
    "                                                  , stratify=y\r\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5952c9",
   "metadata": {},
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a293a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111513\n",
      "7    0.104790\n",
      "3    0.103585\n",
      "9    0.099720\n",
      "2    0.099440\n",
      "6    0.098515\n",
      "0    0.098375\n",
      "4    0.096947\n",
      "8    0.096751\n",
      "5    0.090364\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111587\n",
      "7    0.104762\n",
      "3    0.103651\n",
      "9    0.099683\n",
      "2    0.099524\n",
      "6    0.098413\n",
      "0    0.098413\n",
      "4    0.096984\n",
      "8    0.096667\n",
      "5    0.090317\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Set Distribution\")\r\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\r\n",
    "print('--------------------------------------------------------------')\r\n",
    "print('--------------------------------------------------------------')\r\n",
    "print('--------------------------------------------------------------')\r\n",
    "print(\"Val - Set Distribution\")\r\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a37530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (35700, 784)\n",
      "X_val:  (6300, 784)\n",
      "y_train:  (35700,)\n",
      "y_val:  (6300,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X.shape)\r\n",
    "print(\"X_train: \", X_train.shape)\r\n",
    "print(\"X_val: \", X_val.shape)\r\n",
    "\r\n",
    "print(\"y_train: \", y_train.shape)\r\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92d376",
   "metadata": {},
   "source": [
    "## TK - Free space for coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d78dbc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x251a1140f40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################\r\n",
    "\r\n",
    "plt.imshow(np.asfarray(X_train[2:3]).reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf0175",
   "metadata": {},
   "source": [
    "## Building Transforming Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5be5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import Normalizer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "pipeline = Pipeline([\r\n",
    "    ('normalizer', Normalizer())\r\n",
    "    #('std_scalar',StandardScaler())\r\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839018b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = pipeline.fit_transform(X_train)      # fitting the pipeline to the train and transform it\r\n",
    "X_val_prep = pipeline.transform(X_val)              # transform val data with this information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b7fb8",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network based on RandomizedSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a92d01",
   "metadata": {},
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04790d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\r\n",
    "\r\n",
    "print(\"Relative root_logdir: \",root_logdir)\r\n",
    "\r\n",
    "def get_run_logdir():\r\n",
    "    import time\r\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\r\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a94c97d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_08_25-17_08_41\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\r\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ae86c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_08_25-17_08_41'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f592705",
   "metadata": {},
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9fcc34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54e524",
   "metadata": {},
   "source": [
    "## Building Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20f1bc",
   "metadata": {},
   "source": [
    "### Architecture for Hyperparameter Optimization\n",
    "- Amount of Layers\n",
    "- Amount of Neurons\n",
    "- Learningrate\n",
    "- Checkpoints !!!!!!!!!!!!!!!!!!!!!\n",
    "- Early Stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc28377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[784]):\r\n",
    "    model = keras.models.Sequential()                               # base model structure (Sequential API by Keras)\r\n",
    "\r\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))     # input layer\r\n",
    "\r\n",
    "    for layer in range(n_hidden):                                   # add layers as often as defined in constructor \r\n",
    "        model.add(keras.layers.Dense(n_neurons,activation=\"relu\"))  # add layer with given neurons and relu activation function\r\n",
    "\r\n",
    "    model.add(keras.layers.Dense(10))                               # add output layer \r\n",
    "\r\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)   # define optimizer (especially the larning rate for hyperparameter optimization)\r\n",
    "\r\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)                  # make it ready\r\n",
    "\r\n",
    "    return model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d995533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using keras wrapper as hull \r\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563abc8b",
   "metadata": {},
   "source": [
    "### Hyperparameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd6901a5-c183-483e-9ee5-35da4f9413e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "\n",
    "# Hyperparameter set\n",
    "param_dist= {\n",
    "            \"n_neurons\": range(20, 500, 20)\n",
    "            ,\"n_hidden\": range(10, 100, 10)\n",
    "            ,\"learning_rate\": [1e-3, 2e-3]\n",
    "    }\n",
    "\n",
    "\n",
    "param_dist_lr= {\n",
    "        \"n_neurons\": [100]\n",
    "        ,\"n_hidden\": [10]\n",
    "        ,\"learning_rate\": [1e-2, 2e-3, 1e-3]        # [np.exp(np.log(10**6)/1000)]   #### exp(log(10**6)/500)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_dist_bestrun_1 = {\n",
    "        \"n_neurons\": [150]\n",
    "        ,\"n_hidden\": [30]\n",
    "        ,\"learning_rate\": [2e-3]  \n",
    "}\n",
    "\n",
    "\n",
    "param_dist_bestrun_2 = {\n",
    "        \"n_neurons\": [100]\n",
    "        ,\"n_hidden\": [10]\n",
    "        ,\"learning_rate\": [2e-3]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2cca1a",
   "metadata": {},
   "source": [
    "## TK - Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb8b8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_rans_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34c86c",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8ab38",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6add4da-f980-4590-ac7b-f0e04b65675c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 3 is smaller than n_iter=50. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/744 [..............................] - ETA: 0s - loss: 22.7491WARNING:tensorflow:From D:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0280s). Check your callbacks.\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 5.4307 - val_loss: 2.4217\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.3316 - val_loss: 1.6523\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.5251 - val_loss: 1.7313\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.2204 - val_loss: 1.0568\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9719 - val_loss: 1.0028\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8530 - val_loss: 0.8808\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7843 - val_loss: 0.7780\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7195 - val_loss: 0.8701\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6696 - val_loss: 0.6309\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6191 - val_loss: 0.7754\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5544 - val_loss: 0.7890\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5034 - val_loss: 0.6198\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4971 - val_loss: 2.0133\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4871 - val_loss: 0.5955\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4599 - val_loss: 0.6104\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4050 - val_loss: 0.6090\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4423 - val_loss: 0.8277\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3954 - val_loss: 0.5758\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4335 - val_loss: 0.5573\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3457 - val_loss: 0.5662\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3373 - val_loss: 0.5558\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3154 - val_loss: 0.5557\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3130 - val_loss: 0.5620\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2752 - val_loss: 0.5955\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2631 - val_loss: 1.4711\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2596 - val_loss: 0.5380\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2734 - val_loss: 0.5472\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2200 - val_loss: 0.5626\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2178 - val_loss: 0.5326\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2061 - val_loss: 2.1429\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2394 - val_loss: 0.6848\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2533 - val_loss: 0.5353\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2095 - val_loss: 0.5342\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1890 - val_loss: 0.4820\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2206 - val_loss: 0.5009\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1718 - val_loss: 0.6345\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1966 - val_loss: 0.5441\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2049 - val_loss: 0.5490\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2105 - val_loss: 0.4508\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1489 - val_loss: 0.6967\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1540 - val_loss: 1.2328\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1500 - val_loss: 0.5086\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1225 - val_loss: 0.4761\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1562 - val_loss: 0.5327\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1407 - val_loss: 0.4789\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1390 - val_loss: 0.4779\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1516 - val_loss: 0.4855\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1069 - val_loss: 0.4555\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1117 - val_loss: 0.4547\n",
      "372/372 [==============================] - 0s 569us/step - loss: 0.4213\n",
      "744/744 [==============================] - 0s 553us/step - loss: 0.0834\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:38 - loss: 26.3869WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.4255s). Check your callbacks.\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 5.3596 - val_loss: 2.3679\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.3552 - val_loss: 1.6256\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.6251 - val_loss: 1.8244\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.2185 - val_loss: 0.9239\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0192 - val_loss: 1.8384\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8620 - val_loss: 0.9979\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7679 - val_loss: 0.7786\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6651 - val_loss: 0.6526\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6369 - val_loss: 0.7184\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5916 - val_loss: 1.1541\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5544 - val_loss: 0.6404\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5404 - val_loss: 0.6444\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4896 - val_loss: 0.6499\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4374 - val_loss: 0.5961\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3830 - val_loss: 0.6593\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4059 - val_loss: 0.8287\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3445 - val_loss: 0.6904\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3551 - val_loss: 0.6037\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3404 - val_loss: 0.5724\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2990 - val_loss: 0.6787\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2694 - val_loss: 1.3364\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2735 - val_loss: 0.9265\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2387 - val_loss: 0.5279\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2252 - val_loss: 0.5433\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2225 - val_loss: 0.5242\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2417 - val_loss: 0.6787\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2258 - val_loss: 0.5507\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2039 - val_loss: 0.5543\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2135 - val_loss: 0.5413\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1807 - val_loss: 0.5573\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1702 - val_loss: 0.5110\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1566 - val_loss: 0.5425\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1647 - val_loss: 0.5056\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1346 - val_loss: 0.5326\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2010 - val_loss: 0.5262\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1585 - val_loss: 0.5015\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1804 - val_loss: 0.5338\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1427 - val_loss: 0.5093\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1169 - val_loss: 0.4864\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1048 - val_loss: 0.8969\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1189 - val_loss: 0.4857\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0867 - val_loss: 0.5103\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1011 - val_loss: 0.4889\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2055 - val_loss: 0.5212\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1178 - val_loss: 0.4802\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0877 - val_loss: 0.4920\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0775 - val_loss: 0.4747\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0798 - val_loss: 0.6754\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0756 - val_loss: 0.4942\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0696 - val_loss: 0.4894\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0683 - val_loss: 0.5009\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0655 - val_loss: 0.4851\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0641 - val_loss: 0.4778\n",
      "Epoch 54/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0623 - val_loss: 0.4955\n",
      "Epoch 55/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0608 - val_loss: 0.4736\n",
      "Epoch 56/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0652 - val_loss: 0.4937\n",
      "Epoch 57/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0580 - val_loss: 0.5007\n",
      "Epoch 58/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0577 - val_loss: 0.4927\n",
      "Epoch 59/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0561 - val_loss: 0.4930\n",
      "Epoch 60/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0577 - val_loss: 0.4818\n",
      "Epoch 61/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0852 - val_loss: 0.5308\n",
      "Epoch 62/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2650 - val_loss: 0.6473\n",
      "Epoch 63/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1668 - val_loss: 0.5545\n",
      "Epoch 64/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1622 - val_loss: 0.5141\n",
      "Epoch 65/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1025 - val_loss: 0.5378\n",
      "372/372 [==============================] - 0s 544us/step - loss: 0.5304\n",
      "744/744 [==============================] - 0s 573us/step - loss: 0.1408\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:59 - loss: 29.2173WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.4830s). Check your callbacks.\n",
      "744/744 [==============================] - 2s 2ms/step - loss: 5.7668 - val_loss: 2.6623\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.4866 - val_loss: 2.5647\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.6841 - val_loss: 2.8576\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.2263 - val_loss: 1.1514\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0848 - val_loss: 1.0364\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9050 - val_loss: 1.6885\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8030 - val_loss: 0.7373\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7238 - val_loss: 1.1528\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6964 - val_loss: 0.7205\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6377 - val_loss: 0.9594\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5806 - val_loss: 0.6967\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5435 - val_loss: 0.7852\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4883 - val_loss: 2.2706\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4434 - val_loss: 0.5502\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4200 - val_loss: 0.5232\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4047 - val_loss: 0.5277\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3794 - val_loss: 0.5500\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3637 - val_loss: 0.5361\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3838 - val_loss: 0.5449\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3332 - val_loss: 0.5455\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3212 - val_loss: 0.5452\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2837 - val_loss: 0.5249\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3071 - val_loss: 0.5186\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2628 - val_loss: 0.4720\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2352 - val_loss: 0.4941\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2353 - val_loss: 0.5099\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2111 - val_loss: 0.4780\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2072 - val_loss: 0.5950\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1880 - val_loss: 0.4403\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1949 - val_loss: 0.4708\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1592 - val_loss: 0.4619\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1663 - val_loss: 0.4979\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2000 - val_loss: 0.5606\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1805 - val_loss: 1.4576\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1515 - val_loss: 0.4237\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1863 - val_loss: 0.4181\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2110 - val_loss: 0.4270\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1472 - val_loss: 0.4865\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1146 - val_loss: 0.6635\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2294 - val_loss: 0.7440\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1120 - val_loss: 0.4249\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1124 - val_loss: 0.4474\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0952 - val_loss: 0.5330\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0984 - val_loss: 0.4396\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0820 - val_loss: 0.4606\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0854 - val_loss: 0.4719\n",
      "372/372 [==============================] - 0s 531us/step - loss: 0.4914\n",
      "744/744 [==============================] - 0s 534us/step - loss: 0.0828\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:39 - loss: 29.7234WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.4285s). Check your callbacks.\n",
      "744/744 [==============================] - 2s 2ms/step - loss: 10.1850 - val_loss: 3.5685\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.9575 - val_loss: 4.0865\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.1023 - val_loss: 1.6058\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.5813 - val_loss: 1.4297\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.3368 - val_loss: 1.0908\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.1524 - val_loss: 1.6602\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0129 - val_loss: 1.4307\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9136 - val_loss: 0.8655\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8527 - val_loss: 0.9785\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7670 - val_loss: 0.8287\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7155 - val_loss: 0.7921\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6709 - val_loss: 1.0251\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6279 - val_loss: 0.6965\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5720 - val_loss: 1.0504\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5490 - val_loss: 0.8486\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5152 - val_loss: 0.6377\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 0.4934 - val_loss: 0.6499\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4664 - val_loss: 0.6125\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4533 - val_loss: 0.6433\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4184 - val_loss: 0.7060\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3971 - val_loss: 0.6015\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4061 - val_loss: 0.5918\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3530 - val_loss: 0.6663\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3439 - val_loss: 0.5804\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3344 - val_loss: 0.6068\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3043 - val_loss: 0.5372\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3147 - val_loss: 0.5377\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2784 - val_loss: 0.6279\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3047 - val_loss: 0.7054\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2870 - val_loss: 0.5602\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2487 - val_loss: 0.5437\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2335 - val_loss: 0.5313\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3621 - val_loss: 0.5515\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2136 - val_loss: 0.7299\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2078 - val_loss: 0.5994\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1933 - val_loss: 0.5358\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1919 - val_loss: 0.5169\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1726 - val_loss: 0.5651\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1784 - val_loss: 0.6159\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2269 - val_loss: 0.5816\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1550 - val_loss: 0.5405\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1441 - val_loss: 0.5646\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1610 - val_loss: 0.5248\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1322 - val_loss: 0.7201\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1275 - val_loss: 0.6016\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1318 - val_loss: 0.6199\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1388 - val_loss: 0.5200\n",
      "372/372 [==============================] - 0s 579us/step - loss: 0.4773\n",
      "744/744 [==============================] - 0s 565us/step - loss: 0.1116\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:03 - loss: 31.8581WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.3315s). Check your callbacks.\n",
      "744/744 [==============================] - 2s 2ms/step - loss: 8.5785 - val_loss: 3.5348\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.9529 - val_loss: 2.5799\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.1749 - val_loss: 2.3866\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.7684 - val_loss: 2.2902\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.4710 - val_loss: 1.2829\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.2829 - val_loss: 1.1461\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.1261 - val_loss: 1.5297\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0199 - val_loss: 1.0216\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9211 - val_loss: 0.8902\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8643 - val_loss: 0.9292\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7951 - val_loss: 1.0111\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7269 - val_loss: 0.8508\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6724 - val_loss: 0.7705\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6467 - val_loss: 0.7957\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5975 - val_loss: 0.7114\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5448 - val_loss: 0.7175\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5070 - val_loss: 0.6922\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4795 - val_loss: 0.8272\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4615 - val_loss: 0.8137\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4436 - val_loss: 0.6382\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4166 - val_loss: 0.6799\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3954 - val_loss: 0.6150\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3760 - val_loss: 0.6389\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3434 - val_loss: 0.7700\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3416 - val_loss: 0.6487\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3086 - val_loss: 0.6406\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2911 - val_loss: 0.5951\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2704 - val_loss: 0.7648\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2640 - val_loss: 0.6129\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2501 - val_loss: 0.5938\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2436 - val_loss: 0.5854\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2155 - val_loss: 0.6161\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2255 - val_loss: 0.6143\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1933 - val_loss: 0.6087\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1835 - val_loss: 0.5928\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1953 - val_loss: 0.5650\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1693 - val_loss: 0.5674\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1533 - val_loss: 0.5835\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1502 - val_loss: 0.5727\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1438 - val_loss: 0.6017\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1360 - val_loss: 0.5609\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1283 - val_loss: 0.5665\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1241 - val_loss: 0.6074\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1610 - val_loss: 0.5823\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1156 - val_loss: 0.5936\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1110 - val_loss: 0.5565\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1108 - val_loss: 0.5978\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1090 - val_loss: 0.6495\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1027 - val_loss: 0.5897\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.5896\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0924 - val_loss: 0.5766\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1067 - val_loss: 0.5589\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1011 - val_loss: 0.6206\n",
      "Epoch 54/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0838 - val_loss: 0.5852\n",
      "Epoch 55/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0830 - val_loss: 0.5726\n",
      "Epoch 56/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.5828\n",
      "372/372 [==============================] - 0s 543us/step - loss: 0.5412\n",
      "744/744 [==============================] - 0s 550us/step - loss: 0.0723\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:42 - loss: 22.6455WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.4350s). Check your callbacks.\n",
      "744/744 [==============================] - 2s 2ms/step - loss: 8.7762 - val_loss: 3.3990\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.8137 - val_loss: 2.2791\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.0216 - val_loss: 1.6780\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.5643 - val_loss: 1.3678\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.3328 - val_loss: 1.0834\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.1521 - val_loss: 1.7818\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0331 - val_loss: 0.9251\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9329 - val_loss: 0.8632\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8568 - val_loss: 0.8481\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7675 - val_loss: 0.7665\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7243 - val_loss: 0.8276\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6886 - val_loss: 0.9010\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6174 - val_loss: 0.7679\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5913 - val_loss: 0.6680\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5390 - val_loss: 0.6686\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5215 - val_loss: 0.8027\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4724 - val_loss: 0.6121\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4661 - val_loss: 1.1871\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4463 - val_loss: 0.6673\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4018 - val_loss: 1.8255\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3844 - val_loss: 0.6308\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3712 - val_loss: 0.8343\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3445 - val_loss: 0.5589\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3228 - val_loss: 0.6873\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3044 - val_loss: 0.6814\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2955 - val_loss: 0.5307\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2871 - val_loss: 0.5572\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2782 - val_loss: 0.5533\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2479 - val_loss: 0.5865\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2533 - val_loss: 0.7319\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2218 - val_loss: 0.5637\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2331 - val_loss: 1.2333\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2315 - val_loss: 0.6340\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2165 - val_loss: 0.5374\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1946 - val_loss: 0.5247\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1896 - val_loss: 0.5409\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1636 - val_loss: 0.5666\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1505 - val_loss: 0.5619\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1455 - val_loss: 0.5063\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1405 - val_loss: 0.5341\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1368 - val_loss: 0.5145\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1562 - val_loss: 0.5147\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1305 - val_loss: 0.4985\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1219 - val_loss: 0.5750\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1166 - val_loss: 0.5383\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1552 - val_loss: 0.5593\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1175 - val_loss: 0.5287\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1072 - val_loss: 0.5211\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1060 - val_loss: 0.5418\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1007 - val_loss: 0.5580\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1359 - val_loss: 0.5354\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0977 - val_loss: 0.5064\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.0938 - val_loss: 0.5055\n",
      "372/372 [==============================] - 0s 571us/step - loss: 0.5432\n",
      "744/744 [==============================] - 0s 558us/step - loss: 0.0852\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 2:53 - loss: 29.6007WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.4650s). Check your callbacks.\n",
      "744/744 [==============================] - 2s 2ms/step - loss: 15.4363 - val_loss: 7.2856\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 4.4250 - val_loss: 3.4086\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 3.0565 - val_loss: 2.9820\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.3906 - val_loss: 4.2659\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.9558 - val_loss: 1.7999\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.6581 - val_loss: 1.5148\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.4883 - val_loss: 1.3595\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.3215 - val_loss: 1.2497\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 1.2082 - val_loss: 1.2262\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.1134 - val_loss: 1.0205\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0153 - val_loss: 0.9844\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9836 - val_loss: 1.0149\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8755 - val_loss: 0.8997\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8562 - val_loss: 0.8560\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8163 - val_loss: 0.8634\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7763 - val_loss: 0.9176\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7216 - val_loss: 0.8096\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6853 - val_loss: 0.9723\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6622 - val_loss: 0.7369\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6360 - val_loss: 0.9130\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6138 - val_loss: 0.7086\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5933 - val_loss: 0.7044\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5511 - val_loss: 0.6984\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5358 - val_loss: 0.7539\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5119 - val_loss: 0.7143\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4876 - val_loss: 0.6685\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4760 - val_loss: 0.6421\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 0.4645 - val_loss: 0.7545\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4497 - val_loss: 1.0109\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4248 - val_loss: 0.6355\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4127 - val_loss: 0.6250\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3907 - val_loss: 0.6259\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3835 - val_loss: 0.6531\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 0.3633 - val_loss: 0.6949\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3542 - val_loss: 0.6329\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3425 - val_loss: 0.6092\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3383 - val_loss: 0.6135\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3091 - val_loss: 0.5853\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3127 - val_loss: 0.5897\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 0.2999 - val_loss: 0.6143\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3083 - val_loss: 0.6603\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2867 - val_loss: 0.6086\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2724 - val_loss: 0.5714\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2572 - val_loss: 0.5915\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2498 - val_loss: 0.5810\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2404 - val_loss: 0.6113\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2321 - val_loss: 0.5756\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2435 - val_loss: 0.6895\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2215 - val_loss: 0.6528\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2222 - val_loss: 0.6793\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2141 - val_loss: 0.5596\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2038 - val_loss: 0.5906\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1905 - val_loss: 0.5580\n",
      "Epoch 54/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1904 - val_loss: 0.5562\n",
      "Epoch 55/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1848 - val_loss: 0.5440\n",
      "Epoch 56/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1974 - val_loss: 0.6188\n",
      "Epoch 57/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1775 - val_loss: 0.5882\n",
      "Epoch 58/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1699 - val_loss: 0.5452\n",
      "Epoch 59/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1680 - val_loss: 0.5779\n",
      "Epoch 60/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1606 - val_loss: 0.5749\n",
      "Epoch 61/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1521 - val_loss: 0.5620\n",
      "Epoch 62/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1579 - val_loss: 0.5513\n",
      "Epoch 63/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1447 - val_loss: 0.5499\n",
      "Epoch 64/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1492 - val_loss: 0.6026\n",
      "Epoch 65/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1450 - val_loss: 0.5698\n",
      "372/372 [==============================] - 0s 566us/step - loss: 0.5480\n",
      "744/744 [==============================] - 0s 591us/step - loss: 0.1324\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 3:22 - loss: 23.5187WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.5445s). Check your callbacks.\n",
      "744/744 [==============================] - 2s 2ms/step - loss: 11.5711 - val_loss: 4.1732\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 3.3584 - val_loss: 3.0219\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.5002 - val_loss: 2.0115\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.9764 - val_loss: 2.6824\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.6898 - val_loss: 1.4402\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.4496 - val_loss: 1.3546\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.3247 - val_loss: 1.2005\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.1863 - val_loss: 1.0981\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0743 - val_loss: 1.0218\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9968 - val_loss: 0.9574\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9342 - val_loss: 0.9915\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8847 - val_loss: 1.1624\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8300 - val_loss: 1.2180\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7985 - val_loss: 0.8381\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7356 - val_loss: 0.8211\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 0.7162 - val_loss: 0.8840\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6717 - val_loss: 0.9455\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6375 - val_loss: 0.8150\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6180 - val_loss: 0.7748\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5888 - val_loss: 1.1051\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5711 - val_loss: 0.7400\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5435 - val_loss: 0.7176\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5203 - val_loss: 0.7568\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4875 - val_loss: 0.8489\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4749 - val_loss: 0.6988\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4688 - val_loss: 0.9863\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4414 - val_loss: 0.7034\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4339 - val_loss: 0.7652\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3999 - val_loss: 0.6982\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3911 - val_loss: 0.7020\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3724 - val_loss: 0.6645\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3554 - val_loss: 0.6541\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3643 - val_loss: 0.6658\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3235 - val_loss: 0.6624\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3107 - val_loss: 0.6672\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3111 - val_loss: 0.6910\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2809 - val_loss: 0.6459\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2907 - val_loss: 0.6291\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2597 - val_loss: 0.6560\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2675 - val_loss: 0.6483\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2454 - val_loss: 0.6072\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2500 - val_loss: 0.6358\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2234 - val_loss: 0.6295\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2368 - val_loss: 0.6397\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2147 - val_loss: 0.6234\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1998 - val_loss: 0.6481\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1974 - val_loss: 0.6226\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1955 - val_loss: 0.6193\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1812 - val_loss: 0.6175\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1825 - val_loss: 0.6278\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1816 - val_loss: 0.6108\n",
      "372/372 [==============================] - 0s 551us/step - loss: 0.5679\n",
      "744/744 [==============================] - 0s 578us/step - loss: 0.1464\n",
      "Epoch 1/100\n",
      "  2/744 [..............................] - ETA: 1:51 - loss: 26.3381WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.2995s). Check your callbacks.\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 15.6863 - val_loss: 7.8769\n",
      "Epoch 2/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 5.0961 - val_loss: 3.5858\n",
      "Epoch 3/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.8818 - val_loss: 2.4123\n",
      "Epoch 4/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 2.1945 - val_loss: 2.7372\n",
      "Epoch 5/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.8538 - val_loss: 1.6636\n",
      "Epoch 6/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.5817 - val_loss: 2.1570\n",
      "Epoch 7/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.4428 - val_loss: 1.2472\n",
      "Epoch 8/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.2882 - val_loss: 1.3207\n",
      "Epoch 9/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.1593 - val_loss: 1.1736\n",
      "Epoch 10/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0619 - val_loss: 1.0231\n",
      "Epoch 11/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 1.0200 - val_loss: 1.3221\n",
      "Epoch 12/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9452 - val_loss: 1.4412\n",
      "Epoch 13/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.9036 - val_loss: 0.9228\n",
      "Epoch 14/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8590 - val_loss: 0.8771\n",
      "Epoch 15/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.8132 - val_loss: 0.9150\n",
      "Epoch 16/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7800 - val_loss: 0.8632\n",
      "Epoch 17/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.7135 - val_loss: 0.8538\n",
      "Epoch 18/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6929 - val_loss: 1.1603\n",
      "Epoch 19/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6465 - val_loss: 1.0539\n",
      "Epoch 20/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6250 - val_loss: 0.8235\n",
      "Epoch 21/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.6019 - val_loss: 0.7939\n",
      "Epoch 22/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5926 - val_loss: 0.7295\n",
      "Epoch 23/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5559 - val_loss: 0.8350\n",
      "Epoch 24/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 0.5371 - val_loss: 0.7198\n",
      "Epoch 25/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.5111 - val_loss: 0.6931\n",
      "Epoch 26/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4989 - val_loss: 1.0773\n",
      "Epoch 27/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4820 - val_loss: 0.7358\n",
      "Epoch 28/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4624 - val_loss: 0.6955\n",
      "Epoch 29/100\n",
      "744/744 [==============================] - 1s 2ms/step - loss: 0.4480 - val_loss: 0.7251\n",
      "Epoch 30/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4285 - val_loss: 0.7816\n",
      "Epoch 31/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4132 - val_loss: 0.6505\n",
      "Epoch 32/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.4031 - val_loss: 0.6574\n",
      "Epoch 33/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3809 - val_loss: 0.6886\n",
      "Epoch 34/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3656 - val_loss: 0.6707\n",
      "Epoch 35/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3587 - val_loss: 0.7132\n",
      "Epoch 36/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3322 - val_loss: 0.6345\n",
      "Epoch 37/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3399 - val_loss: 0.7288\n",
      "Epoch 38/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3193 - val_loss: 0.6235\n",
      "Epoch 39/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.3054 - val_loss: 0.6513\n",
      "Epoch 40/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2941 - val_loss: 0.9556\n",
      "Epoch 41/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2796 - val_loss: 0.6253\n",
      "Epoch 42/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2708 - val_loss: 2.2136\n",
      "Epoch 43/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2670 - val_loss: 0.6268\n",
      "Epoch 44/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2534 - val_loss: 0.6438\n",
      "Epoch 45/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2417 - val_loss: 0.5941\n",
      "Epoch 46/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2322 - val_loss: 0.6011\n",
      "Epoch 47/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2272 - val_loss: 0.6593\n",
      "Epoch 48/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2085 - val_loss: 0.6208\n",
      "Epoch 49/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2077 - val_loss: 0.6153\n",
      "Epoch 50/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1976 - val_loss: 0.5999\n",
      "Epoch 51/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1983 - val_loss: 0.5951\n",
      "Epoch 52/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.2024 - val_loss: 0.6560\n",
      "Epoch 53/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1834 - val_loss: 0.6633\n",
      "Epoch 54/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1684 - val_loss: 0.6040\n",
      "Epoch 55/100\n",
      "744/744 [==============================] - 1s 1ms/step - loss: 0.1746 - val_loss: 0.6193\n",
      "372/372 [==============================] - 0s 560us/step - loss: 0.5968\n",
      "744/744 [==============================] - 0s 548us/step - loss: 0.1538\n",
      "Epoch 1/100\n",
      "   2/1116 [..............................] - ETA: 4:08 - loss: 25.6132WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.4445s). Check your callbacks.\n",
      "1116/1116 [==============================] - 2s 2ms/step - loss: 4.6647 - val_loss: 2.3164\n",
      "Epoch 2/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 1.7579 - val_loss: 1.0844\n",
      "Epoch 3/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 1.1991 - val_loss: 1.5855\n",
      "Epoch 4/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.9317 - val_loss: 1.2364\n",
      "Epoch 5/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.8023 - val_loss: 0.6987\n",
      "Epoch 6/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.7010 - val_loss: 0.7641\n",
      "Epoch 7/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.6128 - val_loss: 0.6136\n",
      "Epoch 8/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.5607 - val_loss: 0.6724\n",
      "Epoch 9/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.5332 - val_loss: 0.6012\n",
      "Epoch 10/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.4979 - val_loss: 0.6243\n",
      "Epoch 11/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.4778 - val_loss: 0.5496\n",
      "Epoch 12/100\n",
      "1116/1116 [==============================] - 2s 1ms/step - loss: 0.4322 - val_loss: 0.8392\n",
      "Epoch 13/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.3976 - val_loss: 0.5864\n",
      "Epoch 14/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.3847 - val_loss: 0.5855\n",
      "Epoch 15/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.3691 - val_loss: 0.4474\n",
      "Epoch 16/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.3481 - val_loss: 0.4787\n",
      "Epoch 17/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.3180 - val_loss: 0.5613\n",
      "Epoch 18/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2930 - val_loss: 0.5963\n",
      "Epoch 19/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2809 - val_loss: 0.4335\n",
      "Epoch 20/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.5306\n",
      "Epoch 21/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2812 - val_loss: 0.4773\n",
      "Epoch 22/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2339 - val_loss: 0.4369\n",
      "Epoch 23/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2225 - val_loss: 0.4072\n",
      "Epoch 24/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2216 - val_loss: 0.4747\n",
      "Epoch 25/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.2030 - val_loss: 0.4371\n",
      "Epoch 26/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1948 - val_loss: 0.4689\n",
      "Epoch 27/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1857 - val_loss: 0.4894\n",
      "Epoch 28/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1964 - val_loss: 0.4566\n",
      "Epoch 29/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1989 - val_loss: 0.4054\n",
      "Epoch 30/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1797 - val_loss: 0.4235\n",
      "Epoch 31/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1531 - val_loss: 0.4395\n",
      "Epoch 32/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1533 - val_loss: 0.4456\n",
      "Epoch 33/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1253 - val_loss: 0.4087\n",
      "Epoch 34/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1117 - val_loss: 0.4096\n",
      "Epoch 35/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1185 - val_loss: 0.3701\n",
      "Epoch 36/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1402 - val_loss: 0.3841\n",
      "Epoch 37/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1300 - val_loss: 0.4064\n",
      "Epoch 38/100\n",
      "1116/1116 [==============================] - 2s 1ms/step - loss: 0.1368 - val_loss: 0.4559\n",
      "Epoch 39/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1549 - val_loss: 1.9786\n",
      "Epoch 40/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1409 - val_loss: 0.4196\n",
      "Epoch 41/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1321 - val_loss: 0.4228\n",
      "Epoch 42/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.1172 - val_loss: 0.3903\n",
      "Epoch 43/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.0947 - val_loss: 0.4133\n",
      "Epoch 44/100\n",
      "1116/1116 [==============================] - 1s 1ms/step - loss: 0.0889 - val_loss: 0.4016\n",
      "Epoch 45/100\n",
      "1116/1116 [==============================] - 2s 1ms/step - loss: 0.1271 - val_loss: 0.3763\n"
     ]
    }
   ],
   "source": [
    "# Finding best hyperparameters with Randomized search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "ran_ker_reg = RandomizedSearchCV(keras_reg, param_dist_lr, n_iter=50, cv=3, random_state=seed, return_train_score=True)\n",
    "history_ker_reg = ran_ker_reg.fit(X_train_prep, y_train, epochs=100, validation_data=(X_val_prep, y_val), callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=10), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a2c70f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 100, 'n_hidden': 10, 'learning_rate': 0.01}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_ker_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22ff2842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x00000251A0FDF4C0>,\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'learning_rate': [0.01, 0.002, 0.001],\n",
       "                                        'n_hidden': [10], 'n_neurons': [100]},\n",
       "                   random_state=43, return_train_score=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_ker_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2c024",
   "metadata": {},
   "source": [
    "### TK - Model Training with Full Dataset and Discovered Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "484f42b9-96ca-496e-b701-8f9693d97b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating wrapped regression model with our function.\n",
    "keras_reg_model = keras_reg.build_fn(n_neurons= 100, n_hidden= 10, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "523b47b7-4b21-4d84-8c32-b4502fa46088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 170,410\n",
      "Trainable params: 170,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "765c1468-c5dd-4e41-a5c7-163fb6b277ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new log dir for tensorboard\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_keras_reg_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "452b8f37-1a3e-44e3-8db4-c18c2f77c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data based on our beautifull trained data pipeline\n",
    "X_prep_all = pipeline.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "decd1ca4-2592-420d-aa3f-a81dc5ff92a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25203bcb460>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOkUlEQVR4nO3df6jVdZ7H8de7dlLqTmnrzW4/WG0I+klmB1ktwhg2NAidcJYxGVwIFCqaoYsUs4H6x0JsOzNsYEO2XXRjapqaES2CShPDAulcc8tWNivcGfWWVyImKTXzvX/cb7M3O+dzrt/v95zv0ffzAZdzzvd9vn7enXz5Pfd8zvf7MXcXgNPfGVU3AKAzCDsQBGEHgiDsQBCEHQjibzo52KRJk3zKlCmdHBIIZc+ePTp48KA1qhUKu5nNkfTvks6U9B/u/nDq+VOmTFG9Xi8yJICEWq3WtJb7bbyZnSlplaS5kq6StNDMrsr75wForyK/s8+Q9IG7f+TuRyX9TtK8ctoCULYiYb9Y0p9HPd6bbfsWM1tiZnUzqw8PDxcYDkARRcLe6EOA73z31t1Xu3vN3Wu9vb0FhgNQRJGw75V06ajHl0jaX6wdAO1SJOxvSbrczKaa2VmSfiJpQzltAShb7qk3dz9mZvdKelkjU28D7v5eaZ0BKFWheXZ3f0nSSyX1AqCN+LosEARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0F0dMlm5PPll18m60ePHm1ae+KJJwqN/cYbbyTry5YtS9Z7enqa1q699trkvmYNVx5GThzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAI5tk74MiRI8n64OBgsj579uxk/dixYyfbUmk+/PDD3PX7778/uW9/f3+yPmHChGQd31Yo7Ga2R9Lnkr6WdMzda2U0BaB8ZRzZb3H3gyX8OQDaiN/ZgSCKht0lvWJmg2a2pNETzGyJmdXNrD48PFxwOAB5FQ37je4+XdJcSfeY2c0nPsHdV7t7zd1rvb29BYcDkFehsLv7/uz2gKR1kmaU0RSA8uUOu5mdY2bf/+a+pFsl7SyrMQDlMnfPt6PZZRo5mksjn+o/7e7/ktqnVqt5vV7PNV43O3z4cLK+dOnSZP2pp54qs53TxkUXXZSstzrXfvLkyU1r48ePz9VTt6vVaqrX6w0vBJB76s3dP5J0Xe6uAHQUU29AEIQdCIKwA0EQdiAIwg4EwSmuJXj//feTdabW8tm/f3+yPnXq1GR9/fr1TWu33357rp5OZRzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAI5tnHaPfu3U1rK1eu7GAn5XruueeS9UsuuSRZX758ebL+yiuvnHRPZVm0aFHT2ssvv5zcd+bMmWW3UzmO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBPPsY/TII480ra1bt65prQy33HJLsn7zzd9ZiGfMZs2alaz39fUl6xs2bEjWU5fZXrBgQXLfjRs3JuutHDp0qGltzZo1yX2ZZwdwyiLsQBCEHQiCsANBEHYgCMIOBEHYgSCYZ8+0Wrr6+PHjbRt7y5YtyfqkSZOS9SuvvLLMdk7KWWedlbs+f/785L6vvfZasl7k/8n27duT9bfffjtZv/7663OPXZWWR3YzGzCzA2a2c9S2883sVTPbnd1ObG+bAIoay9v4NZLmnLDtQUmb3P1ySZuyxwC6WMuwu/vrkj49YfM8SWuz+2slzS+3LQBly/sB3WR3H5Kk7PaCZk80syVmVjez+vDwcM7hABTV9k/j3X21u9fcvdbb29vu4QA0kTfsn5hZnyRltwfKawlAO+QN+wZJi7P7iyU1XxsXQFdoOc9uZs9Imi1pkpntlbRc0sOSfm9md0n6k6Qft7PJThgaGkrWBwYG2jb2ddddl6yfe+65bRu7SnfffXeyfsMNNyTrRc45HxwcTNaff/75ZP1UnGdvGXZ3X9ik9MOSewHQRnxdFgiCsANBEHYgCMIOBEHYgSA4xTWzb9++tv3ZEyZMSNbPOIN/cxu5+uqrk/VWr+tnn31WXjOnAf6WAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQzLNn2nka6a233pqsjx8/vm1jn8p6enqS9UWLFiXrq1atyj32s88+m6wvX748WW91ie0qcGQHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSCs1VLFZarVal6v1zs23mhHjhxJ1i+77LJkvdWlpotodd716Xop6aJ27NiRrE+fPr1tY3/xxRfJelXfnajVaqrX69aoxpEdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4IIcz778ePHk/V2zqOjPXp7e6tu4ZTS8shuZgNmdsDMdo7atsLM9pnZjuzntva2CaCosbyNXyNpToPtv3b3adnPS+W2BaBsLcPu7q9L+rQDvQBooyIf0N1rZu9kb/MnNnuSmS0xs7qZ1YeHhwsMB6CIvGH/jaQfSJomaUjSL5s90d1Xu3vN3Wt8oAJUJ1fY3f0Td//a3Y9LekLSjHLbAlC2XGE3s75RD38kaWez5wLoDi3n2c3sGUmzJU0ys72SlkuabWbTJLmkPZKWtq/FcrQ6v/i+++5L1h999NEy2wE6rmXY3X1hg81PtqEXAG3E12WBIAg7EARhB4Ig7EAQhB0IIswprmYNr677V/PmzUvW2zn1tmDBgmT9xRdfTNa7cXngMhw+fDhZb/W6FfHQQw8l6+PGjWvb2O3CkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgggzz97KzJkzk/WbbrqpaW3r1q2Fxt64cWOyPnfu3GR91apVTWtXXHFFrp46odWyx63murdt25Z77LPPPjtZ7+/vT9ZbfW+jG3FkB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgmGfPtLrU9MDAQNPawoWNLsD7/wYHB3P19I3Nmzcn6w888EDT2mOPPVZo7Fbz0V999VXueqvz0YvMo7eyaNGiZP28885r29hV4cgOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0GYu3dssFqt5vV6vWPjdcqbb76ZrM+ZMydZP3ToUJntlOrCCy9M1lv13q3/ba2uQTBr1qwOdVKuWq2mer3e8GT7lkd2M7vUzDab2S4ze8/MfpZtP9/MXjWz3dntxLIbB1CesbyNPyap392vlPT3ku4xs6skPShpk7tfLmlT9hhAl2oZdncfcvft2f3PJe2SdLGkeZLWZk9bK2l+m3oEUIKT+oDOzKZIul7SNkmT3X1IGvkHQdIFTfZZYmZ1M6sPDw8XbBdAXmMOu5n1SPqDpJ+7+1/Gup+7r3b3mrvXent78/QIoARjCruZfU8jQf+tu/8x2/yJmfVl9T5JB9rTIoAytDzF1UaumfukpF3u/qtRpQ2SFkt6OLtd35YOTwGtpmkef/zxZL3V6ZZV+vjjj6tuoamJE9MTQC+88ELTWq1WK7udrjeW89lvlPRTSe+a2Y5s2y80EvLfm9ldkv4k6cdt6RBAKVqG3d23Smp2RfwfltsOgHbh67JAEIQdCIKwA0EQdiAIwg4EwaWkO+COO+5I1u+8885k/emnny6znVNGT09Psr5ly5Zk/ZprrimznVMeR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJ59g4YN25csr5mzZpkvb+/P1lPnbe9YsWK5L6tLiU+cjmD/PuvXLmyaW3ZsmWFxm61zDa+jSM7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBks3AaaTQks0ATg+EHQiCsANBEHYgCMIOBEHYgSAIOxBEy7Cb2aVmttnMdpnZe2b2s2z7CjPbZ2Y7sp/b2t8ugLzGcvGKY5L63X27mX1f0qCZvZrVfu3u/9a+9gCUZSzrsw9JGsruf25muyRd3O7GAJTrpH5nN7Mpkq6XtC3bdK+ZvWNmA2Y2sck+S8ysbmb14eHhYt0CyG3MYTezHkl/kPRzd/+LpN9I+oGkaRo58v+y0X7uvtrda+5e6+3tLd4xgFzGFHYz+55Ggv5bd/+jJLn7J+7+tbsfl/SEpBntaxNAUWP5NN4kPSlpl7v/atT2vlFP+5GkneW3B6AsY/k0/kZJP5X0rpntyLb9QtJCM5smySXtkbS0Df0BKMlYPo3fKqnR+bEvld8OgHbhG3RAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgOrpks5kNS/rfUZsmSTrYsQZOTrf21q19SfSWV5m9/Z27N7z+W0fD/p3BzeruXqusgYRu7a1b+5LoLa9O9cbbeCAIwg4EUXXYV1c8fkq39tatfUn0lldHeqv0d3YAnVP1kR1AhxB2IIhKwm5mc8zsf8zsAzN7sIoemjGzPWb2brYMdb3iXgbM7ICZ7Ry17Xwze9XMdme3DdfYq6i3rljGO7HMeKWvXdXLn3f8d3YzO1PS+5L+QdJeSW9JWuju/93RRpowsz2Sau5e+RcwzOxmSYck/ae7X5Nt+1dJn7r7w9k/lBPd/YEu6W2FpENVL+OdrVbUN3qZcUnzJf2TKnztEn39ozrwulVxZJ8h6QN3/8jdj0r6naR5FfTR9dz9dUmfnrB5nqS12f21GvnL0nFNeusK7j7k7tuz+59L+maZ8Upfu0RfHVFF2C+W9OdRj/equ9Z7d0mvmNmgmS2pupkGJrv7kDTyl0fSBRX3c6KWy3h30gnLjHfNa5dn+fOiqgh7o6Wkumn+70Z3ny5prqR7srerGJsxLePdKQ2WGe8KeZc/L6qKsO+VdOmox5dI2l9BHw25+/7s9oCkdeq+pag/+WYF3ez2QMX9/FU3LePdaJlxdcFrV+Xy51WE/S1Jl5vZVDM7S9JPJG2ooI/vMLNzsg9OZGbnSLpV3bcU9QZJi7P7iyWtr7CXb+mWZbybLTOuil+7ypc/d/eO/0i6TSOfyH8o6Z+r6KFJX5dJ+q/s572qe5P0jEbe1n2lkXdEd0n6W0mbJO3Obs/vot6ekvSupHc0Eqy+inq7SSO/Gr4jaUf2c1vVr12ir468bnxdFgiCb9ABQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBD/B8TFZd9C3je8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test after pipeline use\n",
    "plt.imshow(X_prep_all[1].reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d264db2f-4bed-4d87-b3be-c798d0595772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1313 [..............................] - ETA: 2:11 - loss: 26.7531WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.1985s). Check your callbacks.\n",
      "1291/1313 [============================>.] - ETA: 0s - loss: 4.3645WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 4.3292\n",
      "Epoch 2/100\n",
      "1312/1313 [============================>.] - ETA: 0s - loss: 1.5370WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 1.5366\n",
      "Epoch 3/100\n",
      "1271/1313 [============================>.] - ETA: 0s - loss: 1.0469WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 1.0405\n",
      "Epoch 4/100\n",
      "1304/1313 [============================>.] - ETA: 0s - loss: 0.8325WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.8347\n",
      "Epoch 5/100\n",
      "1313/1313 [==============================] - ETA: 0s - loss: 0.7408WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.7408\n",
      "Epoch 6/100\n",
      "1300/1313 [============================>.] - ETA: 0s - loss: 0.6454WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.6438\n",
      "Epoch 7/100\n",
      "1302/1313 [============================>.] - ETA: 0s - loss: 0.5869WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.5858\n",
      "Epoch 8/100\n",
      "1276/1313 [============================>.] - ETA: 0s - loss: 0.5486WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.5469\n",
      "Epoch 9/100\n",
      "1289/1313 [============================>.] - ETA: 0s - loss: 0.4884WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.4883\n",
      "Epoch 10/100\n",
      "1280/1313 [============================>.] - ETA: 0s - loss: 0.4495WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.4479\n",
      "Epoch 11/100\n",
      "1284/1313 [============================>.] - ETA: 0s - loss: 0.4158WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.4156\n",
      "Epoch 12/100\n",
      "1283/1313 [============================>.] - ETA: 0s - loss: 0.3829WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.3841\n",
      "Epoch 13/100\n",
      "1270/1313 [============================>.] - ETA: 0s - loss: 0.3580WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.3607\n",
      "Epoch 14/100\n",
      "1297/1313 [============================>.] - ETA: 0s - loss: 0.3524WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.3513\n",
      "Epoch 15/100\n",
      "1281/1313 [============================>.] - ETA: 0s - loss: 0.3111WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.3118\n",
      "Epoch 16/100\n",
      "1275/1313 [============================>.] - ETA: 0s - loss: 0.3221WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.3279\n",
      "Epoch 17/100\n",
      "1276/1313 [============================>.] - ETA: 0s - loss: 0.2972WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.2996\n",
      "Epoch 18/100\n",
      "1284/1313 [============================>.] - ETA: 0s - loss: 0.2667WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.2656\n",
      "Epoch 19/100\n",
      "1305/1313 [============================>.] - ETA: 0s - loss: 0.2606WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.2599\n",
      "Epoch 20/100\n",
      "1295/1313 [============================>.] - ETA: 0s - loss: 0.2381WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.2411\n",
      "Epoch 21/100\n",
      "1297/1313 [============================>.] - ETA: 0s - loss: 0.2845WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.2849\n",
      "Epoch 22/100\n",
      "1305/1313 [============================>.] - ETA: 0s - loss: 0.2280WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.2299\n",
      "Epoch 23/100\n",
      "1278/1313 [============================>.] - ETA: 0s - loss: 0.2293WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.2365\n",
      "Epoch 24/100\n",
      "1300/1313 [============================>.] - ETA: 0s - loss: 0.1966WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1959\n",
      "Epoch 25/100\n",
      "1298/1313 [============================>.] - ETA: 0s - loss: 0.2040WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.2041\n",
      "Epoch 26/100\n",
      "1268/1313 [===========================>..] - ETA: 0s - loss: 0.1873WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1845\n",
      "Epoch 27/100\n",
      "1290/1313 [============================>.] - ETA: 0s - loss: 0.1852WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1855\n",
      "Epoch 28/100\n",
      "1276/1313 [============================>.] - ETA: 0s - loss: 0.1606WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1605\n",
      "Epoch 29/100\n",
      "1305/1313 [============================>.] - ETA: 0s - loss: 0.1650WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1649\n",
      "Epoch 30/100\n",
      "1308/1313 [============================>.] - ETA: 0s - loss: 0.1609WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1605\n",
      "Epoch 31/100\n",
      "1312/1313 [============================>.] - ETA: 0s - loss: 0.1567WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1567\n",
      "Epoch 32/100\n",
      "1280/1313 [============================>.] - ETA: 0s - loss: 0.1576WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1594\n",
      "Epoch 33/100\n",
      "1294/1313 [============================>.] - ETA: 0s - loss: 0.1430WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1430\n",
      "Epoch 34/100\n",
      "1298/1313 [============================>.] - ETA: 0s - loss: 0.1768WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1758\n",
      "Epoch 35/100\n",
      "1272/1313 [============================>.] - ETA: 0s - loss: 0.1529WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1521\n",
      "Epoch 36/100\n",
      "1277/1313 [============================>.] - ETA: 0s - loss: 0.1148WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1158\n",
      "Epoch 37/100\n",
      "1273/1313 [============================>.] - ETA: 0s - loss: 0.1238WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1244\n",
      "Epoch 38/100\n",
      "1280/1313 [============================>.] - ETA: 0s - loss: 0.1036WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1051\n",
      "Epoch 39/100\n",
      "1302/1313 [============================>.] - ETA: 0s - loss: 0.1224WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1248\n",
      "Epoch 40/100\n",
      "1285/1313 [============================>.] - ETA: 0s - loss: 0.1155WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1147\n",
      "Epoch 41/100\n",
      "1296/1313 [============================>.] - ETA: 0s - loss: 0.1015WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1016\n",
      "Epoch 42/100\n",
      "1290/1313 [============================>.] - ETA: 0s - loss: 0.0908WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0925\n",
      "Epoch 43/100\n",
      "1278/1313 [============================>.] - ETA: 0s - loss: 0.1003WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1007\n",
      "Epoch 44/100\n",
      "1268/1313 [===========================>..] - ETA: 0s - loss: 0.0808WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0905\n",
      "Epoch 45/100\n",
      "1287/1313 [============================>.] - ETA: 0s - loss: 0.1098WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1094\n",
      "Epoch 46/100\n",
      "1303/1313 [============================>.] - ETA: 0s - loss: 0.1116WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1111\n",
      "Epoch 47/100\n",
      "1273/1313 [============================>.] - ETA: 0s - loss: 0.0748WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0745\n",
      "Epoch 48/100\n",
      "1300/1313 [============================>.] - ETA: 0s - loss: 0.0753WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0749\n",
      "Epoch 49/100\n",
      "1313/1313 [==============================] - ETA: 0s - loss: 0.0618WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0618\n",
      "Epoch 50/100\n",
      "1301/1313 [============================>.] - ETA: 0s - loss: 0.0609WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0611\n",
      "Epoch 51/100\n",
      "1269/1313 [===========================>..] - ETA: 0s - loss: 0.0576WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0577\n",
      "Epoch 52/100\n",
      "1291/1313 [============================>.] - ETA: 0s - loss: 0.0536WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0550\n",
      "Epoch 53/100\n",
      "1286/1313 [============================>.] - ETA: 0s - loss: 0.0537WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0533\n",
      "Epoch 54/100\n",
      "1299/1313 [============================>.] - ETA: 0s - loss: 0.0492WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0493\n",
      "Epoch 55/100\n",
      "1310/1313 [============================>.] - ETA: 0s - loss: 0.0520WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0520\n",
      "Epoch 56/100\n",
      "1283/1313 [============================>.] - ETA: 0s - loss: 0.0474WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0476\n",
      "Epoch 57/100\n",
      "1305/1313 [============================>.] - ETA: 0s - loss: 0.0482WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0484\n",
      "Epoch 58/100\n",
      "1293/1313 [============================>.] - ETA: 0s - loss: 0.0460WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0456\n",
      "Epoch 59/100\n",
      "1286/1313 [============================>.] - ETA: 0s - loss: 0.0437WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0434\n",
      "Epoch 60/100\n",
      "1299/1313 [============================>.] - ETA: 0s - loss: 0.0416WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0422\n",
      "Epoch 61/100\n",
      "1291/1313 [============================>.] - ETA: 0s - loss: 0.0410WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0413\n",
      "Epoch 62/100\n",
      "1308/1313 [============================>.] - ETA: 0s - loss: 0.0417WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0416\n",
      "Epoch 63/100\n",
      "1297/1313 [============================>.] - ETA: 0s - loss: 0.0401WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0404\n",
      "Epoch 64/100\n",
      "1283/1313 [============================>.] - ETA: 0s - loss: 0.0389WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0388\n",
      "Epoch 65/100\n",
      "1281/1313 [============================>.] - ETA: 0s - loss: 0.0392WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0398\n",
      "Epoch 66/100\n",
      "1291/1313 [============================>.] - ETA: 0s - loss: 0.0376WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0372\n",
      "Epoch 67/100\n",
      "1271/1313 [============================>.] - ETA: 0s - loss: 0.0361WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0355\n",
      "Epoch 68/100\n",
      "1272/1313 [============================>.] - ETA: 0s - loss: 0.0363WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0354\n",
      "Epoch 69/100\n",
      "1293/1313 [============================>.] - ETA: 0s - loss: 0.0341WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0342\n",
      "Epoch 70/100\n",
      "1281/1313 [============================>.] - ETA: 0s - loss: 0.0340WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0336\n",
      "Epoch 71/100\n",
      "1273/1313 [============================>.] - ETA: 0s - loss: 0.0337WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0331\n",
      "Epoch 72/100\n",
      "1280/1313 [============================>.] - ETA: 0s - loss: 0.0324WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0328\n",
      "Epoch 73/100\n",
      "1270/1313 [============================>.] - ETA: 0s - loss: 0.0328WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0323\n",
      "Epoch 74/100\n",
      "1285/1313 [============================>.] - ETA: 0s - loss: 0.0322WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0317\n",
      "Epoch 75/100\n",
      "1300/1313 [============================>.] - ETA: 0s - loss: 0.0391WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0390\n",
      "Epoch 76/100\n",
      "1294/1313 [============================>.] - ETA: 0s - loss: 0.0353WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0363\n",
      "Epoch 77/100\n",
      "1308/1313 [============================>.] - ETA: 0s - loss: 0.0341WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0340\n",
      "Epoch 78/100\n",
      "1274/1313 [============================>.] - ETA: 0s - loss: 0.1066WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1060\n",
      "Epoch 79/100\n",
      "1310/1313 [============================>.] - ETA: 0s - loss: 0.1887WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1892\n",
      "Epoch 80/100\n",
      "1293/1313 [============================>.] - ETA: 0s - loss: 0.1103WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1108\n",
      "Epoch 81/100\n",
      "1289/1313 [============================>.] - ETA: 0s - loss: 0.0920WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0920\n",
      "Epoch 82/100\n",
      "1290/1313 [============================>.] - ETA: 0s - loss: 0.0756WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0759\n",
      "Epoch 83/100\n",
      "1281/1313 [============================>.] - ETA: 0s - loss: 0.0481WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0479\n",
      "Epoch 84/100\n",
      "1274/1313 [============================>.] - ETA: 0s - loss: 0.0733WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0744\n",
      "Epoch 85/100\n",
      "1288/1313 [============================>.] - ETA: 0s - loss: 0.0702WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0701\n",
      "Epoch 86/100\n",
      "1293/1313 [============================>.] - ETA: 0s - loss: 0.1972WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1948\n",
      "Epoch 87/100\n",
      "1290/1313 [============================>.] - ETA: 0s - loss: 0.1233WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.1222\n",
      "Epoch 88/100\n",
      "1293/1313 [============================>.] - ETA: 0s - loss: 0.0870WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0875\n",
      "Epoch 89/100\n",
      "1308/1313 [============================>.] - ETA: 0s - loss: 0.1240WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.1236\n",
      "Epoch 90/100\n",
      "1290/1313 [============================>.] - ETA: 0s - loss: 0.0634WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0636\n",
      "Epoch 91/100\n",
      "1293/1313 [============================>.] - ETA: 0s - loss: 0.0394WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0397\n",
      "Epoch 92/100\n",
      "1305/1313 [============================>.] - ETA: 0s - loss: 0.0350WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0349\n",
      "Epoch 93/100\n",
      "1298/1313 [============================>.] - ETA: 0s - loss: 0.0349WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0354\n",
      "Epoch 94/100\n",
      "1269/1313 [===========================>..] - ETA: 0s - loss: 0.0313WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0307\n",
      "Epoch 95/100\n",
      "1305/1313 [============================>.] - ETA: 0s - loss: 0.0296WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0295\n",
      "Epoch 96/100\n",
      "1275/1313 [============================>.] - ETA: 0s - loss: 0.0273WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0285\n",
      "Epoch 97/100\n",
      "1303/1313 [============================>.] - ETA: 0s - loss: 0.0277WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0280\n",
      "Epoch 98/100\n",
      "1305/1313 [============================>.] - ETA: 0s - loss: 0.0277WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 2s 1ms/step - loss: 0.0275\n",
      "Epoch 99/100\n",
      "1302/1313 [============================>.] - ETA: 0s - loss: 0.0271WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0276\n",
      "Epoch 100/100\n",
      "1294/1313 [============================>.] - ETA: 0s - loss: 0.0272WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1313/1313 [==============================] - 1s 1ms/step - loss: 0.0269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25204cbb640>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \n",
    "keras_reg_model.fit(X_prep_all, y, epochs=100, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f6f86",
   "metadata": {},
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa5808",
   "metadata": {},
   "source": [
    "## Peparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7be12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a1b876b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de92ca8",
   "metadata": {},
   "source": [
    "## Creating Competition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "93854396",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02501067",
   "metadata": {},
   "source": [
    "## Prediction of Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "610de436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x251fba5fc70>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test_prep[8].reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f1285359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propability of all lables for given pixels:  [[-0.00202447  0.00639841  0.00208971  0.00351056  0.00654629 -0.00088155\n",
      "   0.00908515  0.00065067  0.00616819  0.00118691]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Propability of all lables for given pixels: \", keras_reg_model.predict(X_test_prep[8].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a2226f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit:  6\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Digit: \",np.argmax(keras_reg_model.predict(X_test_prep[8].reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "69c0d399-cebe-4837-a328-3495eb1e101d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-43a9fbdcf9b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_test_prep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpredicted_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmnist_competition_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_competition_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ImageId'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Label'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1567\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1570\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \"\"\"\n\u001b[0;32m   1694\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4039\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4040\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4041\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4042\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4043\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m-> 2938\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3363\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3364\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mpermutation\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    317\u001b[0m       \u001b[1;31m# than reusing the same range Tensor. (presumably because of buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m       \u001b[1;31m# forwarding.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m       \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   1789\u001b[0m       \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m       \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"limit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1792\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m       \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"delta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1499\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m--> 263\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    264\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    283\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m   \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtensor_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m   const_tensor = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    286\u001b[0m       \"Const\", [], [dtype_value.type], attrs=attrs, name=name).outputs[0]\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         compute_device)\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3475\u001b[0m     \u001b[1;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3476\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3477\u001b[1;33m       ret = Operation(\n\u001b[0m\u001b[0;32m   3478\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3479\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1972\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1974\u001b[1;33m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[0;32m   1975\u001b[0m                                 control_input_ops, op_def)\n\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for row in X_test_prep:\n",
    "    index = i\n",
    "    predicted_label = np.argmax(keras_reg_model.predict(row.reshape(1,-1)))\n",
    "\n",
    "    mnist_competition_file = mnist_competition_file.append({'ImageId': index, 'Label': predicted_label}, ignore_index = True )\n",
    "    i = i + 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5673180",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "290d2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\r\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19421d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
